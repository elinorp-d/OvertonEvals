{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Vanilla LLM Responses\n",
    "### We're going to generate responses from a LLM for each question, based on the habermas_machine_questions.csv file.\n",
    "\n",
    "The input is the habermas_machine_questions.csv file.\n",
    "The output is a large CSV of questions, opinions, and LLM responses. LLM responses & questions are 1-to-1 but duplicated across varying opinions. This is not space efficient but makes it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd, numpy as np, os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "TEMP_PATH = os.getenv('TEMP_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_questions.shape:  (3345, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>perspectives</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>480</td>\n",
       "      <td>Should it be mandatory to recycle household wa...</td>\n",
       "      <td>['I believe it should be mandatory to recycle ...</td>\n",
       "      <td>habermas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>13361</td>\n",
       "      <td>Lying to a cop</td>\n",
       "      <td>['Value: Honesty\\nExplanation: Telling the tru...</td>\n",
       "      <td>valueprism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>226</td>\n",
       "      <td>Is it okay for the government to prohibit peop...</td>\n",
       "      <td>[\"I'm not too sure on this one - on one hand, ...</td>\n",
       "      <td>habermas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>50</td>\n",
       "      <td>Arming the police is a good idea</td>\n",
       "      <td>['It is hypocritical and will not benefit civi...</td>\n",
       "      <td>perspectrum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>28869</td>\n",
       "      <td>run a red light</td>\n",
       "      <td>['Value: Safety\\nExplanation: Running a red li...</td>\n",
       "      <td>valueprism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>690</td>\n",
       "      <td>Should the UK increase the amount of public fu...</td>\n",
       "      <td>[\"Absolutely not. The BBC should make it's own...</td>\n",
       "      <td>habermas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>15895</td>\n",
       "      <td>Pulling a lever to send a train down a track w...</td>\n",
       "      <td>['Value: Preservation of life\\nExplanation: In...</td>\n",
       "      <td>valueprism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>28120</td>\n",
       "      <td>pardoning julian assange</td>\n",
       "      <td>['Value: Justice\\nExplanation: Pardoning Julia...</td>\n",
       "      <td>valueprism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>867</td>\n",
       "      <td>Should the government support businesses by re...</td>\n",
       "      <td>['The government should support businesses by ...</td>\n",
       "      <td>habermas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>663</td>\n",
       "      <td>Should the UK government reduce immigration in...</td>\n",
       "      <td>['i feel the uk government should reduce immig...</td>\n",
       "      <td>habermas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           question  \\\n",
       "480          480  Should it be mandatory to recycle household wa...   \n",
       "2273       13361                                     Lying to a cop   \n",
       "226          226  Is it okay for the government to prohibit peop...   \n",
       "2488          50                   Arming the police is a good idea   \n",
       "2084       28869                                    run a red light   \n",
       "690          690  Should the UK increase the amount of public fu...   \n",
       "2213       15895  Pulling a lever to send a train down a track w...   \n",
       "2228       28120                           pardoning julian assange   \n",
       "867          867  Should the government support businesses by re...   \n",
       "663          663  Should the UK government reduce immigration in...   \n",
       "\n",
       "                                           perspectives       source  \n",
       "480   ['I believe it should be mandatory to recycle ...     habermas  \n",
       "2273  ['Value: Honesty\\nExplanation: Telling the tru...   valueprism  \n",
       "226   [\"I'm not too sure on this one - on one hand, ...     habermas  \n",
       "2488  ['It is hypocritical and will not benefit civi...  perspectrum  \n",
       "2084  ['Value: Safety\\nExplanation: Running a red li...   valueprism  \n",
       "690   [\"Absolutely not. The BBC should make it's own...     habermas  \n",
       "2213  ['Value: Preservation of life\\nExplanation: In...   valueprism  \n",
       "2228  ['Value: Justice\\nExplanation: Pardoning Julia...   valueprism  \n",
       "867   ['The government should support businesses by ...     habermas  \n",
       "663   ['i feel the uk government should reduce immig...     habermas  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions = pd.read_csv(DATA_PATH+'questions_and_human_perspectives.csv')\n",
    "print(\"df_questions.shape: \", df_questions.shape)\n",
    "df_questions.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reduce the number of questions to 100 for testing.\n",
    "df_questions = df_questions.sample(n=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "def generate_responses(questions, generation_function, output_path, start_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    This is a general helper function to generate responses from an LLM and save them to a JSON file. It takes in an arbitrary generation function and can resume from a checkpoint. It will save a JSON file of responses.\n",
    "    \"\"\"\n",
    "    # Load existing responses if any and if we want to resume\n",
    "    responses = {}\n",
    "    if start_from_checkpoint:\n",
    "        with open(output_path, 'r') as f:\n",
    "            responses = json.load(f)\n",
    "\n",
    "    # Make sure the directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Get questions that haven't been answered yet for this model\n",
    "    remaining_questions = [\n",
    "        q for q in questions \n",
    "        if q not in responses\n",
    "    ]\n",
    "        \n",
    "    if not remaining_questions:\n",
    "        print(f\"All questions already processed.\")\n",
    "        return\n",
    "                \n",
    "    # Process each remaining question with progress bar\n",
    "    for idx, question in enumerate(tqdm(remaining_questions, desc=f\"Generating responses\", smoothing=0, ascii=True)):\n",
    "        try:\n",
    "            # Generate response\n",
    "            response = generation_function(question)\n",
    "            \n",
    "            # Store response\n",
    "            responses[question] = response\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing question '{question}' for: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            # Save to JSON\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(responses, f, indent=2)\n",
    "                \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running questions through various LLMs\n",
    "\n",
    "\n",
    "### OpenAI Models\n",
    "We're going to start with OpenAI models. You'll need to set your OpenAI API key in the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|##########| 5/5 [00:37<00:00,  7.42s/it]\n",
      "Generating responses: 100%|##########| 5/5 [00:09<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# Let's generate an LLM response for each question, for each AI model.\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def generate_openai_response(question, model):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# OpenAI models\n",
    "oai_models = ['gpt-4o-mini','gpt-3.5-turbo']\n",
    "\n",
    "for model in oai_models:\n",
    "    output_file = model+'_responses.json'\n",
    "    generation_function = lambda x: generate_openai_response(x, model)\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=df_questions['question'], \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingace Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from huggingface_hub import InferenceClient\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "def query_huggingface(hf_client: InferenceClient, inputs: str, chat:bool=False) -> str:\n",
    "    \"\"\"\n",
    "    This is a helper function to query the Huggingface API.\n",
    "\n",
    "    Huggingface models are either simple inference endpoints, dedicated endpoints, or chat endpoints. We can use dedicated endpoints by passing in the API URL directly.\n",
    "\n",
    "    For non-chat models, we use the text_generation endpoint over the chat_completion endpoint.\n",
    "    \"\"\"\n",
    "    if chat:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": inputs\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        completion =  hf_client.chat_completion(\n",
    "            messages=messages, \n",
    "            max_tokens=1024\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content\n",
    "    else:\n",
    "        completion =  hf_client.text_generation(\n",
    "            prompt=inputs, \n",
    "            max_new_tokens=500\n",
    "        )\n",
    "        return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because many of the non-instruction tuned based models don't have warm inference endpoints (and aren't available on tools like Together), we're going to create dedicated endpoints on AWS through the Huggingface API.\n",
    "# The dedicated endpoints take a while to warm up (also you may need to fiddle with URLs)\n",
    "\n",
    "# models = {\n",
    "#     'gemma-2-2b-it': \"https://z70frgvzih3230r1.us-east-1.aws.endpoints.huggingface.cloud\", \n",
    "#     'gemma-2-2b': \"https://vbkt0rabiunjn175.us-east-1.aws.endpoints.huggingface.cloud\",  \n",
    "#     \"llama-3.1-8B\": \"https://ropkydxq3vq8qff9.us-east-1.aws.endpoints.huggingface.cloud\", # These will be slow to run.\n",
    "#     \"llama-3.1-8B-it\": \"https://dxdj0n50tbi0mgar.us-east-1.aws.endpoints.huggingface.cloud\", \n",
    "# }\n",
    "\n",
    "# non_chat_models = ['llama-3.1-8B', 'gemma-2-2b-it', 'gemma-2-2b']\n",
    "\n",
    "# for model, api_url in models.items():\n",
    "#     output_file = model+'_responses.json'\n",
    "#     hf_client = InferenceClient(model=api_url, token=hf_api_key)\n",
    "#     if model in non_chat_models:\n",
    "#         generation_function = lambda x: query_huggingface(hf_client, x, chat=False)\n",
    "#     else:\n",
    "#         generation_function = lambda x: query_huggingface(hf_client, x, chat=True)\n",
    "\n",
    "#     responses = generate_responses(\n",
    "#         questions=df_questions['question.text'], \n",
    "#         generation_function=generation_function,\n",
    "#         output_path=TEMP_PATH+output_file,\n",
    "#         start_from_checkpoint=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're gonna run this with Google Deepmind Models. You may need to run:\n",
    "\n",
    "`gcloud components update`\n",
    "\n",
    "`gcloud auth application-default login`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with PROJECT_ID:  selfanalysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses:   0%|          | 0/5 [00:00<?, ?it/s]/Users/tobin/miniforge3/lib/python3.10/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "Generating responses: 100%|##########| 5/5 [00:12<00:00,  2.51s/it]\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "# Get the project ID from the .env file\n",
    "PROJECT_ID = os.getenv('GOOGLE_PROJECT_ID')\n",
    "print(\"Running with PROJECT_ID: \", PROJECT_ID)\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "gdp_models = ['gemini-1.5-flash-002']\n",
    "for model in gdp_models:\n",
    "    output_file = model+'_responses.json'\n",
    "    model = GenerativeModel(model)\n",
    "    generation_function = lambda x: model.generate_content(x).text\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=df_questions['question'], \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Together AI Models\n",
    "\n",
    "Together is a great model provider but doesn't have a lot of non-instruction tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is a prompt suggested by Michiel to few shot non-instruction tuned models.\n",
    "# michiel_prompt = lambda x: f\"\"\"Q: What is an ambigram\n",
    "# A: An ambigram is a word, phrase, symbol, or design that retains its meaning or readability when viewed from a different perspective, orientation, or transformation. It is a type of visual wordplay that often relies on symmetry, rotation, or reflection.\n",
    "\n",
    "# Q: What are the zipcodes in Cambridge MA?\n",
    "# A: Cambridge, MA, has multiple ZIP codes depending on the specific area. Here are the primary ones:\n",
    "# 02138: Harvard Square and surrounding areas\n",
    "# 02139: Central Square and parts of MIT\n",
    "# 02140: Porter Square and North Cambridge\n",
    "# 02141: East Cambridge\n",
    "# 02142: Kendall Square and parts of MIT\n",
    "# If you need a ZIP code for a specific address or neighborhood, let me know!\n",
    "\n",
    "# Q: What country is Steve Irwin from?\n",
    "# A: Steve Irwin, famously known as \"The Crocodile Hunter,\" was from Australia. Born in Essendon, Victoria, Australia, he was an internationally renowned wildlife expert, conservationist, and television personality known for his passion for wildlife and environmental preservation.\n",
    "\n",
    "# Q: {x}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|##########| 5/5 [00:14<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.getenv('TOGETHER_API_KEY'))\n",
    "\n",
    "def generate_together_response(question, model):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=[{\"role\": \"user\", \"content\": question}],\n",
    "      max_tokens=2048\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "# While you should be able to use the mistral models on HF, together is much faster with a dedicated endpoint and more models.\n",
    "together_models = {\n",
    "    'mistral-7b-instruct': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "    # 'mistral-7b': 'mistralai/Mistral-7B-v0.1',\n",
    "}\n",
    "\n",
    "for bettername, model in together_models.items():\n",
    "    output_file = bettername+'_responses.json'\n",
    "\n",
    "    if bettername == 'mistral-7b':\n",
    "       nonITprompt = lambda x: f\"Question: {x}\\n\\nAnswer:\"\n",
    "    #    nonITprompt = lambda x: michiel_prompt(x)\n",
    "       generation_function = lambda x: generate_together_response(nonITprompt(x), model)\n",
    "    else:\n",
    "        generation_function = lambda x: generate_together_response(x, model)\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=df_questions['question'], \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Responses\n",
    "#### We're now going to load in all the responses and make them into one big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv(DATA_PATH+'questions_and_human_perspectives.csv') # Load in without and sampling or processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = oai_models + gdp_models + list(together_models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_models = oai_models + gdp_models + list(together_models.keys()) + list(hf_models.keys())\n",
    "# all_models = ['gemma-2-2b-it', 'gemma-2-2b', 'gpt-4o-mini', 'gpt-3.5-turbo', 'gemini-1.5-flash-002', 'mistral-7b-instruct']\n",
    "\n",
    "for model in all_models:\n",
    "    with open(TEMP_PATH+model+'_responses.json', 'r') as f:\n",
    "        model_responses = json.load(f)\n",
    "        df_questions[model] = df_questions['question'].map(model_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.dropna(inplace=True) # Only keep rows where all models have responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to do a basic check to make sure all the model columns are non-null.\n",
    "assert df_questions.isnull().sum().any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.to_csv(DATA_PATH+'questions_and_human_perspectives_with_responses.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
