{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Vanilla LLM Responses\n",
    "### We're going to generate responses from a LLM for each question, based on the habermas_machine_questions.csv file.\n",
    "\n",
    "The input is the habermas_machine_questions.csv file.\n",
    "The output is a large CSV of questions, opinions, and LLM responses. LLM responses & questions are 1-to-1 but duplicated across varying opinions. This is not space efficient but makes it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd, numpy as np, os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "TEMP_PATH = os.getenv('TEMP_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_questions.shape:  (1438, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question.text</th>\n",
       "      <th>own_opinion.text</th>\n",
       "      <th>question_topic</th>\n",
       "      <th>question_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are all lives created equal?</td>\n",
       "      <td>['I feel that all lives may be created equal, ...</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are car manufacturers responsible for the emis...</td>\n",
       "      <td>['They most certainly are we have to be able t...</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are celebrities good role models?</td>\n",
       "      <td>['Some are, some are not. Some are humanitaria...</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are celebrities treated too harshly by the media?</td>\n",
       "      <td>['Mayby,sometimes, they should have some priva...</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are dogs better pets than cats?</td>\n",
       "      <td>['That entirely depends on whether you have an...</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question.text  \\\n",
       "0                       Are all lives created equal?   \n",
       "1  Are car manufacturers responsible for the emis...   \n",
       "2                  Are celebrities good role models?   \n",
       "3  Are celebrities treated too harshly by the media?   \n",
       "4                    Are dogs better pets than cats?   \n",
       "\n",
       "                                    own_opinion.text  question_topic  \\\n",
       "0  ['I feel that all lives may be created equal, ...              84   \n",
       "1  ['They most certainly are we have to be able t...             103   \n",
       "2  ['Some are, some are not. Some are humanitaria...              57   \n",
       "3  ['Mayby,sometimes, they should have some priva...              64   \n",
       "4  ['That entirely depends on whether you have an...              75   \n",
       "\n",
       "   question_id  \n",
       "0            0  \n",
       "1            1  \n",
       "2            2  \n",
       "3            3  \n",
       "4            4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions = pd.read_csv('data/habermas_machine_questions.csv')\n",
    "print(\"df_questions.shape: \", df_questions.shape)\n",
    "df_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reduce the number of questions to 100 for testing.\n",
    "unique_questions = np.random.choice(df_questions['question.text'], 100, replace=False)\n",
    "df_questions = df_questions[df_questions['question.text'].isin(unique_questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "def generate_responses(questions, generation_function, output_path, start_from_checkpoint=True):\n",
    "    \"\"\"\n",
    "    This is a general helper function to generate responses from an LLM and save them to a JSON file. It takes in an arbitrary generation function and can resume from a checkpoint. It will save a JSON file of responses.\n",
    "    \"\"\"\n",
    "    # Load existing responses if any and if we want to resume\n",
    "    responses = {}\n",
    "    if start_from_checkpoint:\n",
    "        with open(output_path, 'r') as f:\n",
    "            responses = json.load(f)\n",
    "\n",
    "    # Get questions that haven't been answered yet for this model\n",
    "    remaining_questions = [\n",
    "        q for q in questions \n",
    "        if q not in responses\n",
    "    ]\n",
    "        \n",
    "    if not remaining_questions:\n",
    "        print(f\"All questions already processed.\")\n",
    "        return\n",
    "                \n",
    "    # Process each remaining question with progress bar\n",
    "    for idx, question in enumerate(tqdm(remaining_questions, desc=f\"Generating responses\", smoothing=0, ascii=True)):\n",
    "        try:\n",
    "            # Generate response\n",
    "            response = generation_function(question)\n",
    "            \n",
    "            # Store response\n",
    "            responses[question] = response\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing question '{question}' for: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        if idx % 1000 == 0:\n",
    "            # Save to JSON\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(responses, f, indent=2)\n",
    "                \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(responses, f, indent=2)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running questions through various LLMs\n",
    "\n",
    "We're going to start with OpenAI models. You'll need to set your OpenAI API key in the .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate an LLM response for each question, for each AI model.\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def generate_openai_response(question, model):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# OpenAI models\n",
    "oai_models = ['gpt-3.5-turbo', 'gpt-4o']\n",
    "\n",
    "for model in oai_models:\n",
    "    output_file = model+'_responses.json'\n",
    "    generation_function = lambda x: generate_openai_response(x, model)\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=df_questions['question.text'], \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're gonna run this with Google Deepmind Models. You may need to run:\n",
    "\n",
    "`gcloud components update`\n",
    "\n",
    "`gcloud auth application-default login`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with PROJECT_ID:  selfanalysis\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "# Get the project ID from the .env file\n",
    "PROJECT_ID = os.getenv('GOOGLE_PROJECT_ID')\n",
    "print(\"Running with PROJECT_ID: \", PROJECT_ID)\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "gdp_models = ['gemini-1.5-flash-002']\n",
    "for model in gdp_models:\n",
    "    output_file = model+'_responses.json'\n",
    "    model = GenerativeModel(model)\n",
    "    generation_function = lambda x: model.generate_content(x).text\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=df_questions['question.text'], \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together(api_key=os.getenv('TOGETHER_API_KEY'))\n",
    "\n",
    "def generate_together_response(question, model):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=[{\"role\": \"user\", \"content\": question}],\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "together_models = {'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo':'llama-3.1-8B', 'google/gemma-2b-it':'gemma-2b-it'}\n",
    "\n",
    "for model, bettername in together_models.items():\n",
    "    output_file = bettername+'_responses.json'\n",
    "    generation_function = lambda x: generate_together_response(x, model)\n",
    "\n",
    "    responses = generate_responses(\n",
    "        questions=df_questions['question.text'], \n",
    "        generation_function=generation_function,\n",
    "        output_path=TEMP_PATH+output_file,\n",
    "        start_from_checkpoint=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Responses\n",
    "#### We're now going to load in all the responses and make them into one big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in oai_models + gdp_models + list(together_models.values()):\n",
    "    with open(TEMP_PATH+model+'_responses.json', 'r') as f:\n",
    "        model_responses = json.load(f)\n",
    "        df_questions[model] = df_questions['question.text'].map(model_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.to_csv('data/habermas_machine_questions_with_responses.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
