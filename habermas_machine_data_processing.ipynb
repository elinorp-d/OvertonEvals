{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGZKXwtiiBiL"
      },
      "source": [
        "# Habermas Machine data and preprocessing\n",
        "This follows the instructions in the [Habermas Machine github repo](https://github.com/google-deepmind/habermas_machine), modified to save the processed questions and opinions.\n",
        "\n",
        "The input to this notebook is the Habermas Machine work, and the output is a dataframe with the questions and opinions `habermas_machine_questions.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-21EkbgjiDtF"
      },
      "source": [
        "# Setup and Importing Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8Bou7ACLh6Yj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'habermas_machine' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Clone github repo locally.\n",
        "!git clone https://github.com/google-deepmind/habermas_machine\n",
        "\n",
        "# Adjust path.\n",
        "import sys\n",
        "sys.path.insert(0,'/content/habermas_machine')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HdDwJRN2iGtW"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import ast\n",
        "import io\n",
        "import requests\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Local imports\n",
        "from habermas_machine.analysis import live_loading, serialise, types\n",
        "\n",
        "# Load helper keys used with dataframes.\n",
        "DFKeys = serialise.SerialisedComparisonKeys\n",
        "DFGroupedKeys = serialise.GroupedSerialisedComparisonKeys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GDRqfi3MiI5x"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(39955, 104)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Load all comparison data from Google Cloud Storage.\n",
        "comparison_data_location = (\n",
        "    'https://storage.googleapis.com/habermas_machine/datasets/hm_all_candidate_comparisons.parquet'\n",
        ")\n",
        "response = requests.get(comparison_data_location)\n",
        "with io.BytesIO(response.content) as f:\n",
        "  df_all = pd.read_parquet(f)\n",
        "clear_output()\n",
        "\n",
        "df_all.shape # Shape of full comparison data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WdBIn4jgiL7z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of participant sessions (before pre-processing): 7189\n",
            "Number of participant sessions (before pre-processing) of each collection:                                       metadata.participant_id\n",
            "metadata.version                                             \n",
            "EVAL_COHORT1_ABLATION_IID_V1                              895\n",
            "EVAL_COHORT2_ABLATION_IID_V2                              990\n",
            "EVAL_COHORT3_ABLATION_OOD_V1                              660\n",
            "EVAL_COHORT4_CRITIQUE_EXCLUSION                           465\n",
            "EVAL_COHORT5_OPINION_EXPOSURE                             425\n",
            "EVAL_COHORT6_HUMAN_MEDIATOR                               540\n",
            "EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK3                      185\n",
            "EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK4                      185\n",
            "EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK5                      180\n",
            "TRAINING_DATA_V1                                         1170\n",
            "TRAINING_DATA_V2                                          675\n",
            "TRAINING_DATA_V3                                          445\n",
            "TRAINING_DATA_V4                                          264\n",
            "TRAINING_DATA_V5                                          110\n"
          ]
        }
      ],
      "source": [
        "#@title Explore data\n",
        "\n",
        "print(\"Number of participant sessions (before pre-processing):\",\n",
        "      df_all[DFKeys.COMPARISON_PARTICIPANT_ID].nunique())\n",
        "\n",
        "print(\n",
        "    \"Number of participant sessions (before pre-processing) of each collection:\",\n",
        "    df_all[\n",
        "        [DFKeys.COMPARISON_VERSION, DFKeys.COMPARISON_PARTICIPANT_ID]\n",
        "    ].drop_duplicates().groupby(DFKeys.COMPARISON_VERSION).count()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WlV_Ye1iOwa"
      },
      "source": [
        "# Example preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bJjl6SLZiNYy"
      },
      "outputs": [],
      "source": [
        "#@title Select dataset\n",
        "\n",
        "dataset_name = 'training' # @param [\"training\", \"cohort1_ablation_iid_v1\", \"cohort2_ablation_iid_v2\", \"cohort3_ablation_ood_v1\", 'cohort4_critique_exclusion', 'cohort5_opinion_exposure', 'cohort6_human_mediator', 'virtual_citizens_assembly']\n",
        "\n",
        "# Set dataset and parameters based on dataset_name.\n",
        "if dataset_name == 'training':\n",
        "  df = df_all[\n",
        "      df_all[DFKeys.COMPARISON_VERSION].isin([\n",
        "          'TRAINING_DATA_V1',\n",
        "          'TRAINING_DATA_V2',\n",
        "          'TRAINING_DATA_V3',\n",
        "          'TRAINING_DATA_V4',\n",
        "          'TRAINING_DATA_V5',\n",
        "      ])\n",
        "  ]\n",
        "  # Backwards incompatibility issue with training data.\n",
        "  df = df.drop(columns=[\n",
        "      DFKeys.CANDIDATES_ALL_REWARD_DATA_WELFARE_OR_RANK,\n",
        "      DFKeys.CANDIDATES_REWARD_DATA_WELFARE_OR_RANK,\n",
        "  ])\n",
        "  min_size_parameters = None\n",
        "  remove_groups_with_repeat_participants = False\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
        "elif dataset_name == 'cohort1_ablation_iid_v1':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT1_ABLATION_IID_V1']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_ABLATION_IID_V1\n",
        "  remove_groups_with_repeat_participants = True\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
        "elif dataset_name == 'cohort2_ablation_iid_v2':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT2_ABLATION_IID_V2']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_ABLATION_IID_V2\n",
        "  remove_groups_with_repeat_participants = True\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
        "elif dataset_name == 'cohort3_ablation_ood_v1':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT3_ABLATION_OOD_V1']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_ABLATION_OOD_V1\n",
        "  remove_groups_with_repeat_participants = True\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
        "elif dataset_name == 'cohort4_critique_exclusion':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT4_CRITIQUE_EXCLUSION']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_CRITIQUE_EXCLUSION\n",
        "  remove_groups_with_repeat_participants = True\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)\n",
        "elif dataset_name == 'cohort5_opinion_exposure':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT5_OPINION_EXPOSURE']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_OPINION_EXPOSURE\n",
        "  remove_groups_with_repeat_participants = False\n",
        "  valid_candidate_provenances = (\n",
        "      types.ResponseProvenance.HUMAN_CITIZEN, # Candidates are other opinions.\n",
        "  )\n",
        "elif dataset_name == 'cohort6_human_mediator':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION] == 'EVAL_COHORT6_HUMAN_MEDIATOR']\n",
        "  min_size_parameters = live_loading.GroupMinSizeParameters.ITERATION_EVAL_HUMAN_MEDIATOR\n",
        "  remove_groups_with_repeat_participants = False\n",
        "  # Candidates can be either model or human statements.\n",
        "  valid_candidate_provenances = (\n",
        "        types.ResponseProvenance.MODEL_MEDIATOR,\n",
        "        types.ResponseProvenance.HUMAN_MEDIATOR, )\n",
        "elif dataset_name == 'virtual_citizens_assembly':\n",
        "  df = df_all[df_all[DFKeys.COMPARISON_VERSION].isin([\n",
        "      'EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK3',\n",
        "      'EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK4',\n",
        "      'EVAL_VIRTUAL_CITIZENS_ASSEMBLY_WEEK5'\n",
        "  ])]\n",
        "  min_size_parameters = None\n",
        "  remove_groups_with_repeat_participants = False\n",
        "  valid_candidate_provenances = (types.ResponseProvenance.MODEL_MEDIATOR,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7G3Ujd2niS6A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing training\n",
            "original df shape (15595, 102)\n",
            "unnested df shape (62380, 102)\n",
            "filtered df shape after removing invalid opinions (56956, 102)\n",
            "filtered df shape after removing invalid candidates (56956, 102)\n",
            "filtered df shape after removing mock ratings (56332, 102)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tobin/Desktop/OvertonEvals/habermas_machine/analysis/live_loading.py:253: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  return df[overlapping_columns].applymap(check_value_unnested).any(axis=None)\n",
            "/Users/tobin/Desktop/OvertonEvals/habermas_machine/analysis/live_loading.py:253: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  return df[overlapping_columns].applymap(check_value_unnested).any(axis=None)\n",
            "/Users/tobin/Desktop/OvertonEvals/habermas_machine/analysis/live_loading.py:253: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  return df[overlapping_columns].applymap(check_value_unnested).any(axis=None)\n",
            "/Users/tobin/Desktop/OvertonEvals/habermas_machine/analysis/live_loading.py:253: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  return df[overlapping_columns].applymap(check_value_unnested).any(axis=None)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "filtered df shape after removing mock rankings (56132, 102)\n",
            "added numerical ratings df shape (56132, 104)\n",
            "renested df shape (14033, 104)\n",
            "Number of groups in preprocessed dataframe: 578\n"
          ]
        }
      ],
      "source": [
        "#@title Example pre-processing\n",
        "print('processing', dataset_name)\n",
        "print('original df shape', df.shape)\n",
        "live_loading.check_consistent_tuple_lengths_in_grouped_columns(\n",
        "    df, groups_columns=[\n",
        "        DFGroupedKeys.OTHER_OPINIONS, DFGroupedKeys.CANDIDATES])\n",
        "\n",
        "# First, unnest columns (e.g., ratings of statements).\n",
        "df_unnested = live_loading.unnest_nested_columns(df)\n",
        "print('unnested df shape', df_unnested.shape)\n",
        "\n",
        "# Remove rows where OWN_OPINION is not HUMAN_CITIZEN (e.g., MOCKs).\n",
        "df_unnested = live_loading.filter_on_response_provenances(\n",
        "    df_unnested,\n",
        "    provenance_column=DFKeys.OWN_OPINION_PROVENANCE,\n",
        "    valid_provenances=(types.ResponseProvenance.HUMAN_CITIZEN,),\n",
        ")\n",
        "print('filtered df shape after removing invalid opinions', df_unnested.shape)\n",
        "\n",
        "# Remove rows where CANDIDATES_PROVEANACE is not as expected:\n",
        "# MODEL_MEDIATOR for most data sets. Can also be HUMAN_CITIZEN or HUMAN_MEDIATOR\n",
        "# for opinion exposure and human mediator comparison, respectively.\n",
        "df_unnested = live_loading.filter_on_response_provenances(\n",
        "    df_unnested,\n",
        "    provenance_column=DFKeys.CANDIDATES_PROVENANCE,\n",
        "    valid_provenances=valid_candidate_provenances,\n",
        ")\n",
        "print('filtered df shape after removing invalid candidates', df_unnested.shape)\n",
        "\n",
        "# Remove mock ratings.\n",
        "df_unnested = live_loading.filter_out_mock_ratings(\n",
        "    df_unnested, rating_type=live_loading.RatingTypes.AGREEMENT)\n",
        "print('filtered df shape after removing mock ratings', df_unnested.shape)\n",
        "\n",
        "# Remove mock rankings.\n",
        "df_unnested = live_loading.filter_out_mock_rankings(df_unnested)\n",
        "print('filtered df shape after removing mock rankings', df_unnested.shape)\n",
        "\n",
        "# Add a column with the numerical equivalents for the Likerts.\n",
        "df_unnested = live_loading.add_numerical_ratings(df_unnested)\n",
        "print('added numerical ratings df shape', df_unnested.shape)\n",
        "\n",
        "# Optional (not used in training or human mediator eval):\n",
        "# Remove groups with repeat participants.\n",
        "if remove_groups_with_repeat_participants:\n",
        "  df_unnested = live_loading.filter_groups_with_repeat_participants(\n",
        "      df_unnested, 'worker_id')\n",
        "  print('filtered df after removing groups with repeat participants', df_unnested.shape)\n",
        "\n",
        "# Renest previously unnested columns.\n",
        "df_nested = live_loading.nest_columns_as_tuples(df_unnested)\n",
        "print('renested df shape', df_nested.shape)\n",
        "\n",
        "# Human Mediator specific preprocessing: Only keep rounds where both human and\n",
        "# model generated statements.\n",
        "if dataset_name == 'cohort6_human_mediator':\n",
        "  df_nested = df_nested[\n",
        "      df_nested[DFKeys.CANDIDATES_PROVENANCE].apply(len) == 2\n",
        "  ]\n",
        "  print('only keeping rounds where both human and model made statement',\n",
        "        df_nested.shape)\n",
        "\n",
        "# Optional: Filter by number of groups of min size (pre-registration criteria).\n",
        "# Note, this should be applied to only a single evaluation dataset and not\n",
        "# multiple datasets at the same time.\n",
        "if min_size_parameters is not None:\n",
        "  df_nested = live_loading.filter_by_number_of_groups_of_min_size(\n",
        "      df_nested,\n",
        "      **min_size_parameters.value)\n",
        "  print('filtered df after setting number of groups of min size', df_nested.shape)\n",
        "\n",
        "print('Number of groups in preprocessed dataframe:',\n",
        "      df_nested[DFKeys.LAUNCH_ID].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "monotonic_timestamp                                                                       1679992968172036\n",
              "critique.metadata.created                                                 2023-03-28T08:14:19.472449+00:00\n",
              "critique.metadata.id                                                      498f61fa95c4cd723dd4aee665118c8d\n",
              "critique.metadata.participant_id                                                                35a875f9b5\n",
              "critique.metadata.provenance                                                                         DUMMY\n",
              "critique.metadata.response_duration                                                                    NaN\n",
              "critique.metadata.status                                                                           PENDING\n",
              "critique.text                                            Dummy participant statement as a filler value ...\n",
              "iteration_id                                                              c5abb209a4aa495d453ea181dc24fe58\n",
              "iteration_index                                                                                          0\n",
              "launch_id                                                                 17b6daea928a41c6a5727aa0c5d674b9\n",
              "metadata.created                                                          2023-03-28T08:14:19.472449+00:00\n",
              "metadata.id                                                               17c9a50fa6bab55c596c31b593f3f267\n",
              "metadata.participant_id                                                                         35a875f9b5\n",
              "metadata.provenance                                                                          HUMAN_CITIZEN\n",
              "metadata.response_duration                                                                       25.308664\n",
              "metadata.status                                                                                  COMPLETED\n",
              "metadata.task_duration                                                                         1708.691818\n",
              "metadata.updated                                                          2023-03-28T08:42:48.164267+00:00\n",
              "metadata.version                                                                          TRAINING_DATA_V1\n",
              "other_opinions.display_label                                                      [None, None, None, None]\n",
              "other_opinions.metadata.created                          [2023-03-28T08:14:19.472449+00:00, 2023-03-28T...\n",
              "other_opinions.metadata.id                               [7a5c677cccfb0c2e5539388c05f1bef0, 7f2605ec99d...\n",
              "other_opinions.metadata.participant_id                    [196beeb338, 6036bc29ed, 49d58e7851, b650c3ef1c]\n",
              "other_opinions.metadata.provenance                       [HUMAN_CITIZEN, HUMAN_CITIZEN, HUMAN_CITIZEN, ...\n",
              "other_opinions.metadata.response_duration                   [66.460169, 236.36231, 187.527486, 249.059361]\n",
              "other_opinions.metadata.status                                [COMPLETED, COMPLETED, COMPLETED, COMPLETED]\n",
              "other_opinions.text                                      [I don't believe they should be allowed to opt...\n",
              "own_opinion.display_label                                                                             None\n",
              "own_opinion.metadata.created                                              2023-03-28T08:14:19.472449+00:00\n",
              "own_opinion.metadata.id                                                   5d65e7df5d7be2b93ea42748b54209c7\n",
              "own_opinion.metadata.participant_id                                                             35a875f9b5\n",
              "own_opinion.metadata.provenance                                                              HUMAN_CITIZEN\n",
              "own_opinion.metadata.response_duration                                                          126.662071\n",
              "own_opinion.metadata.status                                                                      COMPLETED\n",
              "own_opinion.text                                         I dont think parents should be able to opt out...\n",
              "question.affirming_statement                             Parents should be allowed to opt out of sex ed...\n",
              "question.id                                                                                     S173961439\n",
              "question.negating_statement                              Parents should NOT be allowed to opt out of se...\n",
              "question.split                                                                                       TRAIN\n",
              "question.text                                            Should parents be allowed to opt out of sex ed...\n",
              "question.topic                                                                                          66\n",
              "round_id                                                                  315be495e0403a1570c0d3ed7ae548b9\n",
              "timestamp                                                                       2023-03-28 08:42:48.172034\n",
              "top_candidate.aggregation.method                                                                      None\n",
              "top_candidate.all_reward_predictions                                                                  None\n",
              "top_candidate.all_texts                                                                               None\n",
              "top_candidate.display_label                                                                           None\n",
              "top_candidate.metadata.created                                            2023-03-28T08:14:19.472449+00:00\n",
              "top_candidate.metadata.generative_model.api_version                                      dummy_api_version\n",
              "top_candidate.metadata.generative_model.template_name                                       dummy_template\n",
              "top_candidate.metadata.id                                                 092f4f5a78401266e9fe9e52415ff9f3\n",
              "top_candidate.metadata.participant_id                                                                 None\n",
              "top_candidate.metadata.provenance                                                                    DUMMY\n",
              "top_candidate.metadata.response_duration                                                               NaN\n",
              "top_candidate.metadata.reward_model.api_version                                          dummy_api_version\n",
              "top_candidate.metadata.reward_model.template_name                                           dummy_template\n",
              "top_candidate.metadata.status                                                                      PENDING\n",
              "top_candidate.parent_statement_ids                                                                    None\n",
              "top_candidate.prompt_text                                                                             None\n",
              "top_candidate.reward_data.reward_predictions                                                          None\n",
              "top_candidate.text                                       Dummy model statement as a filler value (neede...\n",
              "top_candidate.total_prompt_text                                                                       None\n",
              "top_candidate.all_welfare_or_rank                                                                     None\n",
              "top_candidate.reward_data.welfare_or_rank                                                              NaN\n",
              "worker_id                                                                                             None\n",
              "rankings.metadata.created                                                 2023-03-28T08:14:19.472449+00:00\n",
              "rankings.metadata.id                                                      abd76f0573542b5a6a468f5327cf09a7\n",
              "rankings.metadata.participant_id                                                                35a875f9b5\n",
              "rankings.metadata.provenance                                                                 HUMAN_CITIZEN\n",
              "rankings.metadata.response_duration                                                              25.308664\n",
              "rankings.metadata.status                                                                         COMPLETED\n",
              "ratings.metadata.created                                                  2023-03-28T08:14:19.472449+00:00\n",
              "ratings.metadata.id                                                       162626679ded9a136a9d37af93ed8c4f\n",
              "ratings.metadata.participant_id                                                                 35a875f9b5\n",
              "ratings.metadata.provenance                                                                  HUMAN_CITIZEN\n",
              "ratings.metadata.response_duration                                                               61.711009\n",
              "ratings.metadata.status                                                                          COMPLETED\n",
              "rankings.explanation                                                                                  None\n",
              "candidates.aggregation.method                                                     (None, None, None, None)\n",
              "candidates.all_reward_predictions                                                 (None, None, None, None)\n",
              "candidates.all_texts                                                              (None, None, None, None)\n",
              "candidates.display_label                                                                      (a, b, c, d)\n",
              "candidates.metadata.created                              (2023-03-28T08:14:19.472449+00:00, 2023-03-28T...\n",
              "candidates.metadata.generative_model.api_version          (chinchilla, chinchilla, chinchilla, chinchilla)\n",
              "candidates.metadata.generative_model.template_name       (CITIZEN_JURY_ZERO_SHOT, CITIZEN_JURY_ZERO_SHO...\n",
              "candidates.metadata.id                                   (0807826b74f4425293b8f983c98f3e06, 9ef1b8c9dfc...\n",
              "candidates.metadata.participant_id                                                (None, None, None, None)\n",
              "candidates.metadata.provenance                           (MODEL_MEDIATOR, MODEL_MEDIATOR, MODEL_MEDIATO...\n",
              "candidates.metadata.response_duration                         (11.719726, 11.719726, 11.719726, 11.719726)\n",
              "candidates.metadata.reward_model.api_version                                      (None, None, None, None)\n",
              "candidates.metadata.reward_model.template_name           (CITIZEN_JURY_ZERO_SHOT, CITIZEN_JURY_ZERO_SHO...\n",
              "candidates.metadata.status                                    (COMPLETED, COMPLETED, COMPLETED, COMPLETED)\n",
              "candidates.parent_statement_ids                          ([f5ed054321f5a23a713a1cfd89116206, 7f2605ec99...\n",
              "candidates.prompt_text                                   (A citizen's jury was tasked with coming up wi...\n",
              "candidates.reward_data.reward_predictions                                         (None, None, None, None)\n",
              "candidates.text                                          (In general, the citizen's jury felt that the ...\n",
              "candidates.total_prompt_text                             (A citizen's jury was tasked with coming up wi...\n",
              "rankings.candidate_ids                                   (0807826b74f4425293b8f983c98f3e06, 9ef1b8c9dfc...\n",
              "rankings.numerical_ranks                                                                      (2, 3, 1, 0)\n",
              "ratings.agreement                                               (NEUTRAL, DISAGREE, AGREE, STRONGLY_AGREE)\n",
              "ratings.quality                                          (GOOD_QUALITY, POOR_QUALITY, GOOD_QUALITY, EXC...\n",
              "numerical_ratings.agreement                                                                   (4, 2, 6, 7)\n",
              "numerical_ratings.quality                                                                     (6, 2, 6, 7)\n",
              "Name: 0, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This will be helpful for debugging.\n",
        "pd.set_option('display.max_columns', 400)  # Show all columns\n",
        "pd.set_option('display.max_rows', 400)     # Show all rows\n",
        "\n",
        "# This is an example of what the data looks like.\n",
        "df_nested.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_questions = df_nested[df_nested['iteration_index'] == 0] # This stops us having duplicates from multiple rounds.\n",
        "\n",
        "df_questions = df_questions[['question.text','question.topic','own_opinion.text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question.text</th>\n",
              "      <th>question.topic</th>\n",
              "      <th>own_opinion.text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9990</th>\n",
              "      <td>Are human beings the most intelligent life for...</td>\n",
              "      <td>84</td>\n",
              "      <td>I think it is arrogant to think humans are the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9991</th>\n",
              "      <td>Are human beings the most intelligent life for...</td>\n",
              "      <td>84</td>\n",
              "      <td>So far everything point to humans being the on...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          question.text  question.topic  \\\n",
              "9990  Are human beings the most intelligent life for...              84   \n",
              "9991  Are human beings the most intelligent life for...              84   \n",
              "\n",
              "                                       own_opinion.text  \n",
              "9990  I think it is arrogant to think humans are the...  \n",
              "9991  So far everything point to humans being the on...  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sanity check\n",
        "df_questions[df_questions['question.text'] == 'Are human beings the most intelligent life form in the Universe?']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "grouped_question_to_opinions = df_questions.groupby('question.text')['own_opinion.text']\n",
        "    \n",
        "# Let's check that each question has no duplicates.\n",
        "assert (grouped_question_to_opinions.apply(lambda x: x.count() / x.drop_duplicates().count()) == 1).all()\n",
        "\n",
        "assert (df_questions.groupby('question.text')['question.topic'].nunique() == 1).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_opinions_wrapped = df_questions.groupby('question.text')['own_opinion.text'].apply(lambda x: x.to_list()).to_frame().reset_index()\n",
        "\n",
        "question_to_topic = df_questions.set_index('question.text')['question.topic'].to_dict()\n",
        "\n",
        "df_opinions_wrapped['question_topic'] = df_opinions_wrapped['question.text'].map(question_to_topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's add some unique question ids to make life easier later.\n",
        "df_opinions_wrapped['question_id'] = df_opinions_wrapped.groupby('question.text').ngroup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to csv\n",
        "df_opinions_wrapped.to_csv('data/habermas_machine_questions.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
