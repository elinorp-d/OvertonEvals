{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Entailment\n",
    "\n",
    "How do we know if one text response matches the opinions in another response? This is a tricky question, and there are many ways to approach it. This notebook is going to run through a series of approaches and models, to do some meta-analysis on the best way to do this.\n",
    "\n",
    "It will depend on the `habermas_machine_questions_with_responses.csv` file, which contains the questions and responses generated by the LLMs, as well as the human responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "TEMP_PATH = os.getenv('TEMP_PATH')\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>perspectives</th>\n",
       "      <th>source</th>\n",
       "      <th>gpt-4o-mini</th>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <th>gemini-1.5-flash-002</th>\n",
       "      <th>mistral-7b-instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>442</td>\n",
       "      <td>Should companies be required to pay a living w...</td>\n",
       "      <td>[Most definitely. Everyone has a right to equa...</td>\n",
       "      <td>habermas</td>\n",
       "      <td>The question of whether companies should be re...</td>\n",
       "      <td>There is a strong argument to be made for comp...</td>\n",
       "      <td>There's considerable debate about whether comp...</td>\n",
       "      <td>The concept of a living wage is one that advo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>802</td>\n",
       "      <td>Should the government make it easier for peopl...</td>\n",
       "      <td>[I have never agreed with student loans.  I ne...</td>\n",
       "      <td>habermas</td>\n",
       "      <td>The question of whether the government should ...</td>\n",
       "      <td>Yes, the government should make it easier for ...</td>\n",
       "      <td>The question of making it easier for people st...</td>\n",
       "      <td>Yes, the government should make it easier for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2768</td>\n",
       "      <td>Cannibalising a prostitue</td>\n",
       "      <td>[Value: Respect for life\\nExplanation: Canniba...</td>\n",
       "      <td>valueprism</td>\n",
       "      <td>It seems you may be asking about a sensitive o...</td>\n",
       "      <td>is a horrific act of violence and goes agains...</td>\n",
       "      <td>I cannot provide a response to this query.  Th...</td>\n",
       "      <td>Cannibalism is a taboo and illegal act in mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17705</td>\n",
       "      <td>Saving a man's life</td>\n",
       "      <td>[Value: Preservation of life\\nExplanation: Sav...</td>\n",
       "      <td>valueprism</td>\n",
       "      <td>Saving a man's life can mean different things ...</td>\n",
       "      <td>There are many ways to potentially save a man'...</td>\n",
       "      <td>Saving a man's life is a multifaceted event, a...</td>\n",
       "      <td>Saving a man's life is a critical and potenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15933</td>\n",
       "      <td>Pulling the lever so the trolley crashes into ...</td>\n",
       "      <td>[Value: Well-being\\nExplanation: The action of...</td>\n",
       "      <td>valueprism</td>\n",
       "      <td>The scenario you've presented is reminiscent o...</td>\n",
       "      <td>This hypothetical scenario presents a moral di...</td>\n",
       "      <td>This is a classic trolley problem variation, b...</td>\n",
       "      <td>It is not ethical to consider a scenario wher...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           question  \\\n",
       "0         442  Should companies be required to pay a living w...   \n",
       "1         802  Should the government make it easier for peopl...   \n",
       "2        2768                          Cannibalising a prostitue   \n",
       "3       17705                                Saving a man's life   \n",
       "4       15933  Pulling the lever so the trolley crashes into ...   \n",
       "\n",
       "                                        perspectives      source  \\\n",
       "0  [Most definitely. Everyone has a right to equa...    habermas   \n",
       "1  [I have never agreed with student loans.  I ne...    habermas   \n",
       "2  [Value: Respect for life\\nExplanation: Canniba...  valueprism   \n",
       "3  [Value: Preservation of life\\nExplanation: Sav...  valueprism   \n",
       "4  [Value: Well-being\\nExplanation: The action of...  valueprism   \n",
       "\n",
       "                                         gpt-4o-mini  \\\n",
       "0  The question of whether companies should be re...   \n",
       "1  The question of whether the government should ...   \n",
       "2  It seems you may be asking about a sensitive o...   \n",
       "3  Saving a man's life can mean different things ...   \n",
       "4  The scenario you've presented is reminiscent o...   \n",
       "\n",
       "                                       gpt-3.5-turbo  \\\n",
       "0  There is a strong argument to be made for comp...   \n",
       "1  Yes, the government should make it easier for ...   \n",
       "2   is a horrific act of violence and goes agains...   \n",
       "3  There are many ways to potentially save a man'...   \n",
       "4  This hypothetical scenario presents a moral di...   \n",
       "\n",
       "                                gemini-1.5-flash-002  \\\n",
       "0  There's considerable debate about whether comp...   \n",
       "1  The question of making it easier for people st...   \n",
       "2  I cannot provide a response to this query.  Th...   \n",
       "3  Saving a man's life is a multifaceted event, a...   \n",
       "4  This is a classic trolley problem variation, b...   \n",
       "\n",
       "                                 mistral-7b-instruct  \n",
       "0   The concept of a living wage is one that advo...  \n",
       "1   Yes, the government should make it easier for...  \n",
       "2   Cannibalism is a taboo and illegal act in mos...  \n",
       "3   Saving a man's life is a critical and potenti...  \n",
       "4   It is not ethical to consider a scenario wher...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions = pd.read_csv(DATA_PATH+'questions_and_human_perspectives_with_responses.csv')\n",
    "df_questions['perspectives'] = df_questions['perspectives'].apply(ast.literal_eval)\n",
    "df_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Pulling the lever so the trolley crashes into 100 people but creates 100 million jobs \n",
      " Response: \n",
      " This hypothetical scenario presents a moral dilemma known as the trolley problem. The ethical decision to pull the lever is complex and subjective, as it involves weighing the value of 100 lives against the creation of 100 million jobs.\n",
      "\n",
      "On one hand, saving 100 million jobs could have a significant positive impact on the economy, lifting countless individuals out of poverty and improving their quality of life. This could lead to greater overall societal benefit and long-term stability for a larger number of people.\n",
      "\n",
      "On the other hand, intentionally causing harm to 100 individuals is a difficult choice to make, as each life holds inherent value and should be preserved. The consequences of sacrificing these lives could have far-reaching emotional and psychological effects on their families and communities.\n",
      "\n",
      "Ultimately, the decision to pull the lever depends on an individual's ethical principles, beliefs, and priorities. Some may argue that the greater good outweighs the sacrifice of a few lives, while others may find it morally reprehensible to intentionally harm others for the sake of economic gain. It is a thought-provoking scenario that challenges us to consider the implications of our actions and the values we hold dear. \n",
      "\n",
      " Opinion: \n",
      " Value: Well-being\n",
      "Explanation: The action of creating 100 million jobs could potentially increase the well-being of many people by providing them with employment and financial security.\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at a random question to test the different methods\n",
    "example = df_questions.sample(1).iloc[0]\n",
    "print(\"Question: \", example['question'], \"\\n\", \"Response: \\n\", example['gpt-3.5-turbo'], \"\\n\\n\", \"Opinion: \\n\", example['perspectives'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a constructed example to test the different methods.\n",
    "strawman_question = \"Should we ban or limit access to guns?\"\n",
    "strawman_response_easy = \"No, we should not ban guns. Guns are a necessary tool for self-defense and hunting. We should also make sure that we have a strong police force to protect people and their property.\"\n",
    "strawman_response_hard = \"No, we should not ban guns completely, but we should make it harder for people to get them with strong laws limiting who can buy them.\"\n",
    "strawman_response_mixed = \"While banning guns would result in fewer gun related crimes and accidents, it would also make it harder for people to defend themselves and their property.\"\n",
    "strawman_opinion = \"We should ban guns to reduce crime.\"\n",
    "strawman_opinion_2 = \"Guns are a fundamental right!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Representation\n",
    "\n",
    "These functions are used to determine if the LLM responses match the human opinions (not entailment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, a dead simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to see if the opinions are all represented in the generated responses.\n",
    "def is_my_opinion_represented(question, response, opinion, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Determine if the following opinion is represented in the response to a question. This is the simplest approach I can think of.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Task: Determine if the following opinion is represented in the response to a question.\n",
    "\n",
    "Question: {question}\n",
    "Response: {response}\n",
    "Opinion to check for: {opinion}\n",
    "\n",
    "Instructions:\n",
    "- Answer ONLY with 'yes' or 'no'\n",
    "- Answer 'yes' if the opinion is clearly represented\n",
    "- Answer 'no' if the opinion is absent\n",
    "- Do not explain your reasoning\n",
    "- Do not add any other text\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise evaluator that only responds with 'yes' or 'no'.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        max_tokens=1, # We only need one token for 'yes' or 'no'\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_my_opinion_represented(example['question'], example['gpt-4o-mini'], example['perspectives'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's use a structured CoT to evaluate the representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class OpinionPoint(BaseModel):\n",
    "    claim: str\n",
    "    explicit_matches: list[str]\n",
    "    implicit_matches: list[str]\n",
    "    contradictions: list[str]\n",
    "    coverage_score: int  # 0-10 for this specific point\n",
    "\n",
    "class EvaluationStep(BaseModel):\n",
    "    step_number: int\n",
    "    analysis: str\n",
    "    findings: list[str]\n",
    "\n",
    "class RepresentationAnalysis(BaseModel):\n",
    "    opinion_points: list[OpinionPoint]\n",
    "    evaluation_steps: list[EvaluationStep]\n",
    "    final_score: int  # 0-10 overall score\n",
    "    reasoning: str  # Brief explanation of final score\n",
    "\n",
    "def is_my_opinion_represented_structured_cot(question, response, opinion, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Determine if the opinion is represented in the response to a question, using structured CoT generation.\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"Task: Evaluate how well an opinion is represented in a response through careful step-by-step analysis.\n",
    "\n",
    "Follow these specific steps in your evaluation:\n",
    "1. First, break down the core claims/points in the opinion\n",
    "2. For each point in the opinion:\n",
    "   - Search for explicit mentions in the response\n",
    "   - Look for implicit/paraphrased representations\n",
    "   - Note any contradictions\n",
    "3. Consider the overall alignment:\n",
    "   - How many points are covered?\n",
    "   - How directly are they addressed?\n",
    "   - Are there any misalignments?\n",
    "4. Score the representation from 0-10 where:\n",
    "   - 0: Complete contradiction or no representation\n",
    "   - 1-3: Minimal/weak representation of few points\n",
    "   - 4-6: Partial representation of main points\n",
    "   - 7-9: Strong representation of most points\n",
    "   - 10: Complete and explicit representation of all points\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Question: {question}\n",
    "Response: {response}\n",
    "Opinion to check for: {opinion}\n",
    "\n",
    "Analyze step-by-step following the instructions, then provide your structured evaluation.\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        response_format={\n",
    "            'type': 'json_schema',\n",
    "            'json_schema': \n",
    "                {\n",
    "                \"name\": \"RepresentationChain\", \n",
    "                \"schema\": RepresentationAnalysis.model_json_schema()\n",
    "                }\n",
    "            } \n",
    "    )\n",
    "    \n",
    "    result_object = json.loads(completion.choices[0].message.content)\n",
    "    return result_object\n",
    "\n",
    "def process_representation_result(result_object):\n",
    "    try:\n",
    "        return result_object['final_score']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect how well the LLMs are doing at finding the opinion in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'opinion_points': [{'claim': 'The action of creating 100 million jobs could potentially increase the well-being of many people by providing them with employment and financial security.',\n",
       "   'explicit_matches': ['The creation of jobs might have positive short-term economic impacts.'],\n",
       "   'implicit_matches': ['increasing well-being through employment and financial security'],\n",
       "   'contradictions': [],\n",
       "   'coverage_score': 5}],\n",
       " 'evaluation_steps': [{'step_number': 1,\n",
       "   'analysis': 'The core claim of the opinion is that creating 100 million jobs can enhance well-being by providing employment and financial security.',\n",
       "   'findings': ['The opinion emphasizes the positive impact of job creation on well-being.']},\n",
       "  {'step_number': 2,\n",
       "   'analysis': 'The response acknowledges the potential positive economic impacts of job creation but does not explicitly connect this to the enhancement of well-being. It discusses the moral implications of sacrificing lives for this outcome, which could be seen as a contradiction to the idea of well-being if it involves harm.',\n",
       "   'findings': ['Explicit mention of positive economic impacts from job creation, but no direct link to well-being.']},\n",
       "  {'step_number': 3,\n",
       "   'analysis': \"The response covers the point about job creation but does not fully address the well-being aspect. It focuses more on the ethical implications and potential negative consequences of the action, which may misalign with the opinion's focus on the positive outcomes of job creation.\",\n",
       "   'findings': ['Only one point covered; the direct connection to well-being is weak.']}],\n",
       " 'final_score': 5,\n",
       " 'reasoning': 'The response partially represents the opinion by acknowledging the potential positive economic impacts of job creation, but it does not explicitly connect this to the enhancement of well-being. The focus on ethical implications and potential negative consequences detracts from a strong representation of the opinion.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_my_opinion_represented_structured_cot(example['question'], example['gpt-4o-mini'], example['perspectives'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entailment\n",
    "\n",
    "Entailment is a lot trickier than representation. Our approach is going to be to use the model to find the exact text that matches the opinion, and then we'll see if that text is in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Markdown, display\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# # Generate colors using matplotlib's color map\n",
    "# n_colors = len(spans)\n",
    "# color_map = plt.cm.rainbow  # You can also try: viridis, plasma, magma, etc.\n",
    "# colors = [mpl.colors.rgb2hex(color_map(i / n_colors)) for i in range(n_colors)]\n",
    "\n",
    "\n",
    "# # Generate a style sheet for the spans\n",
    "# style_sheet = \"<style>\\n\"\n",
    "# for perspective_id, _ in enumerate(spans):\n",
    "#     style_sheet += f\"\"\".highlight-{perspective_id} {{ position: relative; }}\n",
    "#     .highlight-{perspective_id}::after {{\n",
    "#         content: \"\";\n",
    "#         position: absolute;\n",
    "#         left: 0; right: 0;\n",
    "#         bottom: -{2*perspective_id}px; /* offset slightly below baseline */\n",
    "#         border-bottom: 2px solid {colors[perspective_id]};\n",
    "#     }}\n",
    "#     \"\"\"\n",
    "# style_sheet += \"</style>\"\n",
    "\n",
    "# # Convert dict to list of (start, end, color) and sort by start position\n",
    "# ordered_span = [(start, end, perspective_id) \n",
    "#                  for perspective_id, span_list in enumerate(spans)\n",
    "#                  for start, end in span_list]\n",
    "# ordered_span.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = []\n",
    "# last_idx = 0\n",
    "\n",
    "# for span_idx, (start, end, perspective_id) in enumerate(ordered_span):\n",
    "#     # Add text before the span\n",
    "#     result.append(text[last_idx:start])\n",
    "#     # Add the highlighted span\n",
    "#     result.append(f\"<span class='highlight-{perspective_id}'>\")\n",
    "#     if span_idx+1 < len(ordered_span) and ordered_span[span_idx+1][0] < end:\n",
    "#         result.append(text[start:end])\n",
    "#     else:\n",
    "#         result.append(text[start:end])\n",
    "#     result.append(\"</span>\")\n",
    "#     last_idx = end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_spans(text: str, spans: list[list[tuple[int, int]]]):\n",
    "    \"\"\"\n",
    "    Highlight the spans in the text based on the spans dictionary. \n",
    "    Args:\n",
    "        text: The text to highlight\n",
    "        # spans_dict: Dictionary of 'color': list of spans, where each span is (start, end)\n",
    "    Example:\n",
    "        spans = [\n",
    "            [(0, 4), (10, 16)],\n",
    "            [(28, 37)]\n",
    "        }\n",
    "    \"\"\"\n",
    "    from IPython.display import Markdown, display\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "\n",
    "    # Generate colors using matplotlib's color map\n",
    "    n_colors = len(spans)\n",
    "    color_map = plt.cm.rainbow  # You can also try: viridis, plasma, magma, etc.\n",
    "    colors = [mpl.colors.rgb2hex(color_map(i / n_colors)) for i in range(n_colors)]\n",
    "\n",
    "    # Create the spans dictionary using these colors\n",
    "    spans_dict = {color: spans[i] for i, color in enumerate(colors)}\n",
    "    # highlight_spans(text, spans_dict)\n",
    "\n",
    "    # Generate a style sheet for the spans\n",
    "    style_sheet = \"<style>\"\n",
    "    for span_id, span in enumerate(spans):\n",
    "        style_sheet += f\"\"\".highlight-{span_id} {{ background-color: {colors[span_id]}; }}\n",
    "        .highlight-{span_id}::after {{\n",
    "            content: \"\";\n",
    "            position: absolute;\n",
    "            left: 0; right: 0;\n",
    "            bottom: -{2*span_id}px; /* offset slightly below baseline */\n",
    "            border-bottom: 2px solid {colors[span_id]};\n",
    "        }}\n",
    "        \"\"\"\n",
    "    style_sheet += \"</style>\"\n",
    "    \n",
    "    # Convert dict to list of (start, end, color) and sort by start position\n",
    "    all_spans = [(start, end, color) \n",
    "                 for color, spans in spans_dict.items() \n",
    "                 for start, end in spans]\n",
    "    all_spans.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Build the marked up text piece by piece\n",
    "    result = []\n",
    "    last_idx = 0\n",
    "    \n",
    "    for start, end, color in all_spans:\n",
    "        # Add text before the span\n",
    "        result.append(text[last_idx:start])\n",
    "        # Add the highlighted span\n",
    "        result.append(f\"<span style='color: {color};'>{text[start:end]}</span>\")\n",
    "        last_idx = end\n",
    "    \n",
    "    # Add any remaining text after the last span\n",
    "    result.append(text[last_idx:])\n",
    "    \n",
    "    # Add color swatches at the end\n",
    "    for color in spans_dict:\n",
    "        swatch = f\"<span style='color: {color};'>{chr(9608) * 6}</span>\"\n",
    "        result.append(f\" {swatch}\")\n",
    "    \n",
    "    # Join all pieces and display\n",
    "    marked_text = ''.join(result)\n",
    "    display(Markdown(marked_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured CoT (OpenAI) for entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use the writefile magic to save the entailment code to a file for use downsteam and for version control.\n",
    "# %%writefile src/entailment.py\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import openai, os, json\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "class EntailmentMatch(BaseModel):\n",
    "    text: str\n",
    "    match_type: str  # \"direct\", \"paraphrase\", or \"contextual\"\n",
    "    confidence: int  # 0-10 score\n",
    "    explanation: str  # Why this is a match\n",
    "\n",
    "class EntailmentStep(BaseModel):\n",
    "    step_number: int\n",
    "    concept: str  # The concept from the opinion being analyzed\n",
    "    analysis: str  # The reasoning process\n",
    "    matches: list[EntailmentMatch]\n",
    "\n",
    "class EntailmentAnalysis(BaseModel):\n",
    "    steps: list[EntailmentStep]\n",
    "    final_matches: list[str]  # The best, most confident matches\n",
    "    coverage_score: int  # 0-10 how well the opinion is covered\n",
    "\n",
    "def entailment_from_gpt_json(question: str, response: str, opinion: str, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Find exact text matches between rich text and opinion using GPT-4.\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"Task: Precise Text Entailment Analysis. Find and evaluate text in the Response that represents concepts from the Opinion.\n",
    "\n",
    "Follow these specific steps:\n",
    "1. Break down the Opinion into key concepts\n",
    "2. For each concept:\n",
    "   - Search for direct text matches, this includes single words like \"yes\" or \"no\"\n",
    "   - Identify paraphrased representations\n",
    "   - Look for contextual/implicit matches\n",
    "   - Copy the **exact text** in the Response that matches the concept in the Opinion. Copy the text from the response, not the opinion.\n",
    "\n",
    "3. Evaluate matches by:\n",
    "   - Precision: How exactly does it match?\n",
    "   - Context: Is the meaning preserved?\n",
    "   - Completeness: Is the full concept captured?\n",
    "\n",
    "4. Score coverage from 0-10 where:\n",
    "   - 0: No valid matches found\n",
    "   - 1-3: Few weak/partial matches\n",
    "   - 4-6: Some good matches but incomplete\n",
    "   - 7-9: Strong matches for most concepts\n",
    "   - 10: Complete, precise matches for all concepts\n",
    "\n",
    "Important:\n",
    "- Prioritize precision over quantity\n",
    "- Consider context to avoid false matches\n",
    "- Explain reasoning for each match\n",
    "- Always copy the exact text from the Response that matches the concept\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Context question: {question}\n",
    "Opinion: {opinion}\n",
    "Response: {response}\n",
    "\n",
    "Analyze step-by-step following the instructions to find and evaluate all relevant matches.\"\"\"\n",
    "    \n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        response_format={\n",
    "            'type': 'json_schema',\n",
    "            'json_schema': \n",
    "                {\n",
    "                \"name\": \"EntailmentAnalysis\", \n",
    "                \"schema\": EntailmentAnalysis.model_json_schema()\n",
    "                }\n",
    "            } \n",
    "    )\n",
    "\n",
    "    result_object = json.loads(chat_response.choices[0].message.content)\n",
    "    return result_object\n",
    "\n",
    "def process_entailment_result(result_object, response):\n",
    "    matches = []\n",
    "    for match in result_object['final_matches']:\n",
    "        start_index = response.lower().find(match.lower())\n",
    "        if start_index == -1:\n",
    "            print(\"Warning: match was not found in response text.\")\n",
    "            continue\n",
    "        end_index = start_index + len(match)\n",
    "        matches.append((start_index, end_index))\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': [{'step_number': 1,\n",
       "   'concept': 'Value: Well-being',\n",
       "   'analysis': \"The concept of well-being is represented in the Response through the discussion of the positive impact of creating jobs on individuals' lives and the economy.\",\n",
       "   'matches': [{'text': 'could have a significant positive impact on the economy, lifting countless individuals out of poverty and improving their quality of life.',\n",
       "     'match_type': 'Paraphrased representation',\n",
       "     'confidence': 8,\n",
       "     'explanation': \"This text captures the essence of well-being by discussing how job creation can lift individuals out of poverty and improve their quality of life, which aligns with the Opinion's focus on well-being.\"}]},\n",
       "  {'step_number': 2,\n",
       "   'concept': 'Creating 100 million jobs',\n",
       "   'analysis': 'The Response discusses the creation of jobs in the context of the trolley problem, emphasizing the potential benefits of such an action.',\n",
       "   'matches': [{'text': 'saving 100 million jobs could have a significant positive impact on the economy',\n",
       "     'match_type': 'Paraphrased representation',\n",
       "     'confidence': 7,\n",
       "     'explanation': \"This phrase paraphrases the concept of creating jobs by discussing the positive impact of saving jobs, although it does not explicitly state 'creating 100 million jobs'.\"}]},\n",
       "  {'step_number': 3,\n",
       "   'concept': 'Employment and financial security',\n",
       "   'analysis': \"The Response implies the connection between job creation and financial security, though it does not explicitly mention 'financial security'.\",\n",
       "   'matches': [{'text': 'lifting countless individuals out of poverty',\n",
       "     'match_type': 'Contextual match',\n",
       "     'confidence': 6,\n",
       "     'explanation': 'This phrase implies financial security as it suggests that individuals will have better economic conditions, which is a component of financial security.'}]},\n",
       "  {'step_number': 4,\n",
       "   'concept': 'Increase well-being of many people',\n",
       "   'analysis': 'The Response discusses the broader societal benefits of job creation, which can be interpreted as an increase in well-being for many.',\n",
       "   'matches': [{'text': 'greater overall societal benefit and long-term stability for a larger number of people',\n",
       "     'match_type': 'Paraphrased representation',\n",
       "     'confidence': 7,\n",
       "     'explanation': \"This text suggests an increase in well-being for many by discussing societal benefits and stability, which aligns with the Opinion's focus on well-being.\"}]}],\n",
       " 'final_matches': ['could have a significant positive impact on the economy, lifting countless individuals out of poverty and improving their quality of life.',\n",
       "  'saving 100 million jobs could have a significant positive impact on the economy',\n",
       "  'lifting countless individuals out of poverty',\n",
       "  'greater overall societal benefit and long-term stability for a larger number of people'],\n",
       " 'coverage_score': 7}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entailment_results = entailment_from_gpt_json(example['question'], example['gpt-3.5-turbo'], example['perspectives'][0])\n",
    "entailment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(275, 413), (251, 330), (332, 376), (433, 519)]\n"
     ]
    }
   ],
   "source": [
    "entailment_matches = process_entailment_result(entailment_results, example['gpt-3.5-turbo'])\n",
    "print(entailment_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(75, 155), (0, 73)]\n"
     ]
    }
   ],
   "source": [
    "# strawman_entailment_results = entailment_from_gpt_json(strawman_question, strawman_response_mixed, strawman_opinion)\n",
    "# strawman_entailment_matches = process_entailment_result(strawman_entailment_results, strawman_response_mixed)\n",
    "# print(strawman_entailment_matches)\n",
    "\n",
    "strawman_entailment_results_2 = entailment_from_gpt_json(strawman_question, strawman_response_mixed, strawman_opinion_2)\n",
    "strawman_entailment_matches_2 = process_entailment_result(strawman_entailment_results_2, strawman_response_mixed)\n",
    "print(strawman_entailment_matches_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This hypothetical scenario presents a moral dilemma known as the trolley problem. The ethical decision to pull the lever is complex and subjective, as it involves weighing the value of 100 lives against the creation of 100 million jobs.\n",
       "\n",
       "On one hand, <span style='color: #8000ff;'>saving 100 million jobs could have a significant positive impact on the economy</span><span style='color: #8000ff;'>could have a significant positive impact on the economy, lifting countless individuals out of poverty and improving their quality of life.</span><span style='color: #8000ff;'>lifting countless individuals out of poverty</span> and improving their quality of life. This could lead to <span style='color: #8000ff;'>greater overall societal benefit and long-term stability for a larger number of people</span>.\n",
       "\n",
       "On the other hand, intentionally causing harm to 100 individuals is a difficult choice to make, as each life holds inherent value and should be preserved. The consequences of sacrificing these lives could have far-reaching emotional and psychological effects on their families and communities.\n",
       "\n",
       "Ultimately, the decision to pull the lever depends on an individual's ethical principles, beliefs, and priorities. Some may argue that the greater good outweighs the sacrifice of a few lives, while others may find it morally reprehensible to intentionally harm others for the sake of economic gain. It is a thought-provoking scenario that challenges us to consider the implications of our actions and the values we hold dear. <span style='color: #8000ff;'>██████</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highlight_spans(example['gpt-3.5-turbo'], [entailment_matches])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [00:15<00:05,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: match was not found in response text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [00:19<00:03,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: match was not found in response text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:21<00:00,  3.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# Now we're going to run this across all the opinions and highlight each match in the response.\n",
    "\n",
    "opinion_entailments = []\n",
    "for opinion_idx, opinion in enumerate(tqdm(example['perspectives'])):\n",
    "    opinion_entailments.append({})\n",
    "    opinion_entailments[opinion_idx]['full_result'] = entailment_from_gpt_json(example['question'], example['gpt-4o-mini'], opinion)\n",
    "    opinion_entailments[opinion_idx]['matches'] = process_entailment_result(opinion_entailments[opinion_idx]['full_result'], example['gpt-4o-mini'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The scenario you've presented is reminiscent of the classic ethical dilemma known as the \"trolley problem,\" which explores <span style='color: #ff6d38;'>the moral implications of making choices that affect the lives of others</span>. In this case, <span style='color: #386df9;'>pulling the lever results in a tragic loss of life</span> but <span style='color: #8000ff;'>leads to a significant positive outcome in the form of job creation</span><span style='color: #386df9;'>leads to a significant positive outcome in the form of job creation</span><span style='color: #12c8e6;'>leads to a significant positive outcome in the form of job creation</span>.\n",
       "\n",
       "It's important to note that <span style='color: #12c8e6;'>while consequentialist ethics might argue that the end justifies the means</span><span style='color: #12c8e6;'>the end justifies the means (in this case, sacrificing 100 people for the greater good of creating 100 million jobs)</span><span style='color: #ff6d38;'>the greater good of creating 100 million jobs</span>), many ethical frameworks would strongly oppose such a decision. Here are a few key points to consider in this discussion:\n",
       "\n",
       "1. **Moral Responsibility**: <span style='color: #386df9;'>The act of intentionally causing harm, even for a perceived greater good, raises questions of moral responsibility and accountability</span><span style='color: #12c8e6;'>The act of intentionally causing harm, even for a perceived greater good, raises questions of moral responsibility and accountability</span><span style='color: #5af8c8;'>The act of intentionally causing harm, even for a perceived greater good, raises questions of moral responsibility and accountability.</span><span style='color: #ecc86f;'>The act of intentionally causing harm, even for a perceived greater good, raises questions of moral responsibility and accountability.</span>\n",
       "\n",
       "2. **Value of Human Life**: Many ethical systems, including deontological ethics, hold that <span style='color: #386df9;'>human life has inherent value and should not be sacrificed for utilitarian gains</span><span style='color: #5af8c8;'>human life has inherent value and should not be sacrificed for utilitarian gains.</span>\n",
       "\n",
       "3. **Long-term Consequences**: <span style='color: #8000ff;'>The creation of jobs might have positive short-term economic impacts</span><span style='color: #ff6d38;'>positive short-term economic impacts</span>, but the societal repercussions of such a tragedy (grief, loss, potential retaliatory actions) could have long-lasting negative effects.\n",
       "\n",
       "4. **Alternative Solutions**: Ethical decision-making often involves exploring alternatives that do not require sacrificing lives. Are there ways to create jobs that do not involve such dire costs?\n",
       "\n",
       "5. **Public Perception and Trust**: <span style='color: #12c8e6;'>Actions that lead to the loss of life for economic gain</span> could lead to a breakdown of trust in systems or leaders, ultimately undermining social cohesion.\n",
       "\n",
       "In conclusion, while the scenario poses an interesting hypothetical for discussing ethics and decision-making, the actual implications are complex and multifaceted, and many would argue against any course of action that results in the deliberate harm of individuals, regardless of the potential for broader societal benefits. <span style='color: #8000ff;'>██████</span> <span style='color: #386df9;'>██████</span> <span style='color: #12c8e6;'>██████</span> <span style='color: #5af8c8;'>██████</span> <span style='color: #a4f89f;'>██████</span> <span style='color: #ecc86f;'>██████</span> <span style='color: #ff6d38;'>██████</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Highlight the matches for the example\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Generate colors using matplotlib's color map\n",
    "n_colors = len(opinion_entailments)\n",
    "color_map = plt.cm.rainbow  # You can also try: viridis, plasma, magma, etc.\n",
    "colors = [mpl.colors.rgb2hex(color_map(i / n_colors)) for i in range(n_colors)]\n",
    "\n",
    "# Create the spans dictionary using these colors\n",
    "spans_dict = {color: opinion_entailments[i]['matches'] for i, color in enumerate(colors)}\n",
    "highlight_spans(example['gpt-4o-mini'], list(spans_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MetaAI SONAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sonar-space in ./.venv/lib/python3.11/site-packages (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.21 in ./.venv/lib/python3.11/site-packages (from sonar-space) (2.1.3)\n",
      "Requirement already satisfied: sox in ./.venv/lib/python3.11/site-packages (from sonar-space) (1.5.0)\n",
      "Requirement already satisfied: soundfile in ./.venv/lib/python3.11/site-packages (from sonar-space) (0.12.1)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from sonar-space) (4.67.1)\n",
      "Requirement already satisfied: overrides in ./.venv/lib/python3.11/site-packages (from sonar-space) (7.7.0)\n",
      "Requirement already satisfied: typing_extensions in ./.venv/lib/python3.11/site-packages (from sonar-space) (4.12.2)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.11/site-packages (from soundfile->sonar-space) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.11/site-packages (from cffi>=1.0->soundfile->sonar-space) (2.22)\n",
      "Collecting fairseq2\n",
      "  Downloading fairseq2-0.2.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting fairseq2n==0.2.1 (from fairseq2)\n",
      "  Downloading fairseq2n-0.2.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (948 bytes)\n",
      "Collecting jiwer~=3.0 (from fairseq2)\n",
      "  Downloading jiwer-3.0.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting numpy~=1.23 (from fairseq2)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Requirement already satisfied: overrides~=7.3 in ./.venv/lib/python3.11/site-packages (from fairseq2) (7.7.0)\n",
      "Collecting packaging~=23.1 (from fairseq2)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pyyaml~=6.0 in ./.venv/lib/python3.11/site-packages (from fairseq2) (6.0.2)\n",
      "Collecting sacrebleu~=2.3 (from fairseq2)\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: torch>=1.12.1 in ./.venv/lib/python3.11/site-packages (from fairseq2) (2.5.1)\n",
      "Collecting torcheval~=0.0.6 (from fairseq2)\n",
      "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: tqdm~=4.62 in ./.venv/lib/python3.11/site-packages (from fairseq2) (4.67.1)\n",
      "Collecting torch>=1.12.1 (from fairseq2)\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch>=1.12.1->fairseq2) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.12.1->fairseq2) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch>=1.12.1->fairseq2) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.12.1->fairseq2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.12.1->fairseq2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch>=1.12.1->fairseq2) (2024.10.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in ./.venv/lib/python3.11/site-packages (from jiwer~=3.0->fairseq2) (8.1.7)\n",
      "Collecting rapidfuzz<4,>=3 (from jiwer~=3.0->fairseq2)\n",
      "  Downloading rapidfuzz-3.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting portalocker (from sacrebleu~=2.3->fairseq2)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in ./.venv/lib/python3.11/site-packages (from sacrebleu~=2.3->fairseq2) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.11/site-packages (from sacrebleu~=2.3->fairseq2) (0.9.0)\n",
      "Collecting colorama (from sacrebleu~=2.3->fairseq2)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lxml (from sacrebleu~=2.3->fairseq2)\n",
      "  Downloading lxml-5.3.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.12.1->fairseq2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch>=1.12.1->fairseq2) (1.3.0)\n",
      "Downloading fairseq2-0.2.1-py3-none-any.whl (191 kB)\n",
      "Downloading fairseq2n-0.2.1-cp311-cp311-macosx_14_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp311-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiwer-3.0.5-py3-none-any.whl (21 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
      "Downloading rapidfuzz-3.11.0-cp311-cp311-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading lxml-5.3.0-cp311-cp311-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: torcheval, rapidfuzz, portalocker, packaging, numpy, lxml, colorama, torch, sacrebleu, jiwer, fairseq2n, fairseq2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "Successfully installed colorama-0.4.6 fairseq2-0.2.1 fairseq2n-0.2.1 jiwer-3.0.5 lxml-5.3.0 numpy-1.26.4 packaging-23.2 portalocker-3.0.0 rapidfuzz-3.11.0 sacrebleu-2.4.3 torch-2.2.2 torcheval-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install sonar-space\n",
    "!pip install fairseq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
    "# t2vec_model = TextToEmbeddingModelPipeline(encoder=\"text_sonar_basic_encoder\",\n",
    "#                                            tokenizer=\"text_sonar_basic_encoder\")\n",
    "# sentences = ['My name is SONAR.', 'I can embed the sentences into vectorial space.']\n",
    "# embeddings = t2vec_model.predict(sentences, source_lang=\"eng_Latn\")\n",
    "# print(embeddings.shape)\n",
    "# # torch.Size([2, 1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using real NLI methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch\n",
    "# !pip install sentence-splitter\n",
    "# !pip install sentence-transformers\n",
    "# !pip install tiktoken\n",
    "# !pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobin/Desktop/OvertonEvals/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While banning guns would result in fewer gun related crimes and accidents, it would also make it harder for people to defend themselves and their property.\n",
      "We should ban guns to reduce crime.\n",
      "{'entailment': 0.7, 'neutral': 9.8, 'contradiction': 89.6}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(strawman_response_mixed)\n",
    "print(strawman_opinion)\n",
    "input = tokenizer(strawman_response_mixed, strawman_opinion_2, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a paragraph.', 'It contains several sentences.', '\"But why,\" you ask?']\n"
     ]
    }
   ],
   "source": [
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "\n",
    "text = \"This is a paragraph. It contains several sentences. \\\"But why,\\\" you ask?\"\n",
    "sentences = split_text_into_sentences(text=text, language='en')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We could also validate with facebook/xnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge-mnli\")\n",
    "NLI_model = transformers.pipeline(\"text-classification\", model=\"microsoft/deberta-v2-xlarge-mnli\", tokenizer=tokenizer, top_k=None, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While banning guns would result in fewer gun related crimes and accidents, it would also make it harder for people to defend themselves and their property.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strawman_response_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'This is a complex and debated topic. While some argue that a universal basic income or monthly allowance from the government could provide financial security to all individuals and reduce inequality, others worry about the potential impact on work incentives and the economy as a whole. Ultimately, whether or not all adults should be given a monthly allowance by the government is a decision that must be carefully weighed and considered in the context of broader economic and social policies.',\n",
       " 'text_pair': 'Yes because if we were given an allowance it would encourage people to spend which would aid the economy and lead to better investment within the UK.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"text\": example['gpt-3.5-turbo'], \"text_pair\": example['own_opinion.text'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-comparing the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "Questions:  33%|███▎      | 1/3 [00:48<01:36, 48.26s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'final_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: match was not found in response text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "Questions:  67%|██████▋   | 2/3 [01:09<00:32, 32.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: match was not found in response text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "Questions: 100%|██████████| 3/3 [01:56<00:00, 38.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# I apologize for how chaotic and unreadable this cell is, but we want to run all the same functions and save all intermediate results for debugging.\n",
    "\n",
    "entailment_models = ['gpt-4o-mini']\n",
    "response_models = ['gpt-3.5-turbo']\n",
    "\n",
    "overton_results = []\n",
    "sample_size = 3\n",
    "for _, row in tqdm(df_questions.sample(sample_size).iterrows(), total=sample_size, desc=\"Questions\", leave=True):\n",
    "    question = row['question.text']\n",
    "    opinions = row['own_opinion.text']\n",
    "    question_id = row['question_id']\n",
    "    with tqdm(total=len(opinions), desc=\"Opinions\", leave=False) as opinion_bar:\n",
    "        for opinion_idx, opinion in enumerate(opinions):\n",
    "            with tqdm(total=len(response_models), desc=\"Response Models\", leave=False) as response_bar:\n",
    "                for response_model in response_models:\n",
    "                    with tqdm(total=len(entailment_models), desc=\"Entailment Models\", leave=False) as entailment_bar:\n",
    "                        for entailment_model in entailment_models:\n",
    "                            response = row[response_model]\n",
    "                            # This is the simple prompt check (which should be the same for all models)\n",
    "                            result_opinion_represented = is_my_opinion_represented(question, response, opinion, model=entailment_model)\n",
    "                            # This is the structured cot check, which should be the same for all models\n",
    "                            result_opinion_represented_structured_cot = is_my_opinion_represented_structured_cot(question, response, opinion, model=entailment_model)\n",
    "                            result_opinion_represented_structured_cot_score = process_representation_result(result_opinion_represented_structured_cot)\n",
    "\n",
    "\n",
    "                            # Now we turn to entailment, which we're going to save in a bunch of different formats for debugging\n",
    "                            entailment_result = entailment_from_gpt_json(question, response, opinion, model=entailment_model)\n",
    "                            entailment_matches = process_entailment_result(entailment_result, response)\n",
    "\n",
    "                            overton_results.append({\n",
    "                                'question_id': question_id,\n",
    "                                'opinion_idx': opinion_idx,\n",
    "                                'response_model': response_model,\n",
    "                                'entailment_model': entailment_model,\n",
    "                                'is_represented_simple_prompt': result_opinion_represented == 'yes',\n",
    "                                'is_represented_structured_cot': result_opinion_represented_structured_cot,\n",
    "                                'is_represented_structured_cot_score': result_opinion_represented_structured_cot_score,\n",
    "                                'entailment_result': entailment_result,\n",
    "                                'entailment_matches': entailment_matches\n",
    "                            })\n",
    "                            entailment_bar.update(1)\n",
    "                    response_bar.update(1)\n",
    "            opinion_bar.update(1)\n",
    "\n",
    "df_overton_results = pd.DataFrame(overton_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>opinion_idx</th>\n",
       "      <th>response_model</th>\n",
       "      <th>entailment_model</th>\n",
       "      <th>is_represented_simple_prompt</th>\n",
       "      <th>is_represented_structured_cot</th>\n",
       "      <th>is_represented_structured_cot_score</th>\n",
       "      <th>entailment_result</th>\n",
       "      <th>entailment_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>True</td>\n",
       "      <td>{'evaluation_steps': [{'step_number': 1, 'anal...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>{'steps': [{'step_number': 1, 'concept': 'dece...</td>\n",
       "      <td>[(15, 137), (347, 435), (175, 267), (459, 542)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>False</td>\n",
       "      <td>{'evaluation_steps': [{'step_number': 1, 'anal...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>{'steps': [{'step_number': 1, 'concept': 'limi...</td>\n",
       "      <td>[(0, 138), (139, 268)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>True</td>\n",
       "      <td>{'evaluation_steps': [{'step_number': 1, 'anal...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>{'steps': [{'step_number': 1, 'concept': 'limi...</td>\n",
       "      <td>[(0, 138), (283, 436), (139, 268)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>True</td>\n",
       "      <td>{'evaluation_steps': [{'step_number': 1, 'anal...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>{'steps': [{'step_number': 1, 'concept': 'Limi...</td>\n",
       "      <td>[(5, 137), (175, 267), (175, 267), (437, 542)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>True</td>\n",
       "      <td>{'evaluation_steps': [{'step_number': 1, 'anal...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>{'steps': [{'step_number': 1, 'concept': 'limi...</td>\n",
       "      <td>[(0, 138), (139, 268), (175, 268), (269, 436),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id  opinion_idx response_model entailment_model  \\\n",
       "0           34            0  gpt-3.5-turbo      gpt-4o-mini   \n",
       "1           34            1  gpt-3.5-turbo      gpt-4o-mini   \n",
       "2           34            2  gpt-3.5-turbo      gpt-4o-mini   \n",
       "3           34            3  gpt-3.5-turbo      gpt-4o-mini   \n",
       "4           34            4  gpt-3.5-turbo      gpt-4o-mini   \n",
       "\n",
       "   is_represented_simple_prompt  \\\n",
       "0                          True   \n",
       "1                         False   \n",
       "2                          True   \n",
       "3                          True   \n",
       "4                          True   \n",
       "\n",
       "                       is_represented_structured_cot  \\\n",
       "0  {'evaluation_steps': [{'step_number': 1, 'anal...   \n",
       "1  {'evaluation_steps': [{'step_number': 1, 'anal...   \n",
       "2  {'evaluation_steps': [{'step_number': 1, 'anal...   \n",
       "3  {'evaluation_steps': [{'step_number': 1, 'anal...   \n",
       "4  {'evaluation_steps': [{'step_number': 1, 'anal...   \n",
       "\n",
       "   is_represented_structured_cot_score  \\\n",
       "0                                  6.0   \n",
       "1                                  4.0   \n",
       "2                                  9.0   \n",
       "3                                  4.0   \n",
       "4                                  6.0   \n",
       "\n",
       "                                   entailment_result  \\\n",
       "0  {'steps': [{'step_number': 1, 'concept': 'dece...   \n",
       "1  {'steps': [{'step_number': 1, 'concept': 'limi...   \n",
       "2  {'steps': [{'step_number': 1, 'concept': 'limi...   \n",
       "3  {'steps': [{'step_number': 1, 'concept': 'Limi...   \n",
       "4  {'steps': [{'step_number': 1, 'concept': 'limi...   \n",
       "\n",
       "                                  entailment_matches  \n",
       "0    [(15, 137), (347, 435), (175, 267), (459, 542)]  \n",
       "1                             [(0, 138), (139, 268)]  \n",
       "2                 [(0, 138), (283, 436), (139, 268)]  \n",
       "3     [(5, 137), (175, 267), (175, 267), (437, 542)]  \n",
       "4  [(0, 138), (139, 268), (175, 268), (269, 436),...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_overton_results = pd.DataFrame(overton_results)\n",
    "df_overton_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results.to_csv('data/entailment_ablation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results = pd.read_csv('data/entailment_ablation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "gpt-3.5-turbo    0.066667\n",
       "gpt-4o           0.200000\n",
       "gpt-4o-mini      0.266667\n",
       "Name: is_represented_simple_prompt, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_overton_results.groupby(['model', 'question_id'])['is_represented_simple_prompt'].mean().reset_index().groupby('model')['is_represented_simple_prompt'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results['is_represented_structured_cot_score.bool'] = df_overton_results['is_represented_structured_cot_score'] > 5\n",
    "df_overton_results['meta_analysis.simpleXstructured'] = (df_overton_results['is_represented_simple_prompt'] == df_overton_results['is_represented_structured_cot_score.bool']).astype(int)\n",
    "df_overton_results['meta_analysis.entailmentLength'] = df_overton_results['entailment_matches'].apply(lambda x: len(x))\n",
    "df_overton_results['meta_analysis.entailmentXstructured'] = ((df_overton_results['meta_analysis.entailmentLength'] > 1) == df_overton_results['is_represented_structured_cot_score.bool'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta analysis:\n",
      "- Percent represented (structured cot) > 5: 0.3333333333333333\n",
      "- Percent represented (simple prompt) == Percent represented (structured cot): 0.8333333333333334\n",
      "- Percent entailment length > 1 == represented (structured cot): 0.4166666666666667\n",
      "- Mean Entailment Length: 3.4166666666666665\n",
      "- Max Entailment Length: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Meta analysis:\n",
    "- Percent represented (structured cot) > 5: {df_overton_results['is_represented_structured_cot_score.bool'].mean()}\n",
    "- Percent represented (simple prompt) == Percent represented (structured cot): {df_overton_results['meta_analysis.simpleXstructured'].mean()}\n",
    "- Percent entailment length > 1 == represented (structured cot): {df_overton_results['meta_analysis.entailmentXstructured'].mean()}\n",
    "- Mean Entailment Length: {df_overton_results['meta_analysis.entailmentLength'].mean()}\n",
    "- Max Entailment Length: {df_overton_results['meta_analysis.entailmentLength'].max()}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "example =df_overton_results.sample(1).iloc[0]\n",
    "example_question = df_questions.loc[df_questions['question_id'] == example['question_id']].iloc[0]\n",
    "opinion = example_question['own_opinion.text'][example['opinion_idx']]\n",
    "response =example_question[example['response_model']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Are there any limits on what can be allowed to be broadcast on television?\n",
      "      \n",
      "Response: Yes, there are regulations and guidelines set by government agencies and industry groups that dictate what can be broadcast on television. These regulations typically include restrictions on airing explicit content such as nudity, violence, profanity, and hate speech. Additionally, there are rules on the timing of certain types of content, with more stringent regulations in place during times when children are likely to be watching. Networks and channels must comply with these regulations in order to maintain their broadcasting licenses.\n",
      "\n",
      "From model: gpt-3.5-turbo\n",
      "\n",
      "Opinion: I think there are absolutely limits to what can be broadcast on television. I do like the current system we have in my country which has rules for pre and post watershed, so that the rules are stricter for times when children may be watching television. I do not consider myself a prude and think adults can make up their own mind on what is acceptable to watch but overall though I think that censorship of some kind is needed to ensure that people are not exposed to things that society considers abhorrent.\n",
      "\n",
      "ANALYSIS:\n",
      "\n",
      "Is represented (simple prompt): True\n",
      "\n",
      "Is represented (structured cot): {'evaluation_steps': [{'step_number': 1, 'analysis': 'The core claims in the opinion include: 1) There are limits on what can be broadcast on television. 2) The current system of pre and post watershed rules is effective. 3) Adults should have the freedom to choose what to watch. 4) Some level of censorship is necessary to protect society from abhorrent content.', 'findings': ['Limits exist on broadcast content', 'Support for pre and post watershed rules', 'Adults should have viewing freedom', 'Censorship is necessary for societal protection']}, {'step_number': 2, 'analysis': \"1) The response explicitly mentions regulations and guidelines that dictate what can be broadcast, which aligns with the opinion's claim about limits. 2) The response discusses the timing of content and stricter regulations during times when children are likely to be watching, which aligns with the opinion's support for the watershed system. 3) The response does not explicitly address the opinion's point about adults making their own choices, but it implies that regulations exist for societal protection. 4) The response supports the idea of censorship by discussing the need for regulations to prevent exposure to explicit content, aligning with the opinion's view on necessary censorship.\", 'findings': ['Explicit mention of limits on broadcast content', 'Explicit mention of watershed rules', 'Implicit representation of adult viewing freedom', 'Support for censorship in the response']}, {'step_number': 3, 'analysis': \"The response covers all four main points of the opinion. The first two points are directly addressed, while the third point is implicitly represented. The fourth point about censorship is also supported in the response. There are no contradictions present in the response, and it aligns well with the opinion's overall sentiment.\", 'findings': ['All main points covered', 'Directly addressed points', 'Implicit representation of adult freedom', 'No contradictions found']}], 'opinion_points': [{'claim': 'There are limits on what can be broadcast on television.', 'explicit_matches': ['Yes, there are regulations and guidelines set by government agencies and industry groups that dictate what can be broadcast on television.'], 'implicit_matches': [], 'contradictions': [], 'coverage_score': 10}, {'claim': 'The current system of pre and post watershed rules is effective.', 'explicit_matches': ['rules for pre and post watershed'], 'implicit_matches': ['rules are stricter for times when children may be watching'], 'contradictions': [], 'coverage_score': 10}, {'claim': 'Adults should have the freedom to choose what to watch.', 'explicit_matches': [], 'implicit_matches': ['implies adults can make choices within regulations'], 'contradictions': [], 'coverage_score': 7}, {'claim': 'Some level of censorship is necessary to protect society from abhorrent content.', 'explicit_matches': ['censorship of some kind is needed to ensure that people are not exposed to things that society considers abhorrent.'], 'implicit_matches': [], 'contradictions': [], 'coverage_score': 10}], 'final_score': 9, 'reasoning': 'The response effectively represents the opinion, covering all main points with strong explicit and implicit matches. The only slight weakness is the representation of adult viewing freedom, which is implied rather than explicitly stated. Overall, the response aligns well with the opinion, justifying a high score.'}\n",
      "\n",
      "Entailment result: [(0, 138), (283, 436), (139, 268)]\n",
      "\n",
      "Entailment Reasoning: {'steps': [{'step_number': 1, 'concept': 'limits on what can be broadcast on television', 'analysis': \"The Response confirms that there are regulations and guidelines that dictate what can be broadcast on television, which aligns with the Opinion's assertion that there are limits.\", 'matches': [{'text': 'Yes, there are regulations and guidelines set by government agencies and industry groups that dictate what can be broadcast on television.', 'match_type': 'Direct Match', 'confidence': 9, 'explanation': 'This text directly states that there are regulations that dictate what can be broadcast, matching the concept of limits.'}]}, {'step_number': 2, 'concept': 'rules for pre and post watershed', 'analysis': \"The Response mentions rules on the timing of certain types of content, which corresponds to the Opinion's reference to pre and post watershed rules.\", 'matches': [{'text': 'there are rules on the timing of certain types of content, with more stringent regulations in place during times when children are likely to be watching.', 'match_type': 'Paraphrased Representation', 'confidence': 8, 'explanation': \"This paraphrases the concept of pre and post watershed rules by discussing timing and stricter regulations during children's viewing times.\"}]}, {'step_number': 3, 'concept': 'censorship is needed', 'analysis': \"The Response implies that censorship is necessary by discussing the restrictions on explicit content, which aligns with the Opinion's view that some censorship is needed.\", 'matches': [{'text': 'These regulations typically include restrictions on airing explicit content such as nudity, violence, profanity, and hate speech.', 'match_type': 'Contextual Match', 'confidence': 7, 'explanation': \"This text implies a form of censorship by stating that there are restrictions on explicit content, which aligns with the Opinion's belief in the need for censorship.\"}]}, {'step_number': 4, 'concept': 'society considers abhorrent', 'analysis': 'The Response does not explicitly mention societal views on what is considered abhorrent, but it does discuss regulations that reflect societal standards.', 'matches': [{'text': 'These regulations typically include restrictions on airing explicit content such as nudity, violence, profanity, and hate speech.', 'match_type': 'Contextual Match', 'confidence': 6, 'explanation': \"While it does not directly state 'society considers abhorrent', the mention of restrictions reflects societal standards.\"}]}], 'final_matches': ['Yes, there are regulations and guidelines set by government agencies and industry groups that dictate what can be broadcast on television.', 'there are rules on the timing of certain types of content, with more stringent regulations in place during times when children are likely to be watching.', 'These regulations typically include restrictions on airing explicit content such as nudity, violence, profanity, and hate speech.'], 'coverage_score': 8}\n",
      "\n",
      "META ANALYSIS:\n",
      "\n",
      "Simple prompt == Structured cot: True\n",
      "\n",
      "Entailment length > 1 AND simple prompt == structured cot: True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Question: {example_question['question.text']}\n",
    "      \n",
    "Response: {response}\n",
    "\n",
    "From model: {example['response_model']}\n",
    "\n",
    "Opinion: {opinion}\n",
    "\n",
    "ANALYSIS:\n",
    "\n",
    "Is represented (simple prompt): {example['is_represented_simple_prompt']}\n",
    "\n",
    "Is represented (structured cot): {example['is_represented_structured_cot']}\n",
    "\n",
    "Entailment result: {example['entailment_matches']}\n",
    "\n",
    "Entailment Reasoning: {example['entailment_result']}\n",
    "\n",
    "META ANALYSIS:\n",
    "\n",
    "Simple prompt == Structured cot: {example['meta_analysis.simpleXstructured'] == 1}\n",
    "\n",
    "Entailment length > 1 AND simple prompt == structured cot: {example['meta_analysis.entailmentXstructured']}\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
