{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Entailment\n",
    "\n",
    "How do we know if one text response matches the opinions in another response? This is a tricky question, and there are many ways to approach it. This notebook is going to run through a series of approaches and models, to do some meta-analysis on the best way to do this.\n",
    "\n",
    "It will depend on the `habermas_machine_questions_with_responses.csv` file, which contains the questions and responses generated by the LLMs, as well as the human responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question.text</th>\n",
       "      <th>own_opinion.text</th>\n",
       "      <th>question_topic</th>\n",
       "      <th>question_id</th>\n",
       "      <th>gemma-2-2b-it</th>\n",
       "      <th>gemma-2-2b</th>\n",
       "      <th>gpt-4o-mini</th>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <th>gemini-1.5-flash-002</th>\n",
       "      <th>mistral-7b-instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are the NHS and the UK welfare state fit for p...</td>\n",
       "      <td>[The NHS and Welfare are not working how they ...</td>\n",
       "      <td>51</td>\n",
       "      <td>19</td>\n",
       "      <td>[{'generated_text': \"Are the NHS and the UK we...</td>\n",
       "      <td>[{'generated_text': 'Are the NHS and the UK we...</td>\n",
       "      <td>The question of whether the NHS and the UK wel...</td>\n",
       "      <td>This is a highly debated and subjective issue....</td>\n",
       "      <td>Whether the NHS and the UK welfare state are \"...</td>\n",
       "      <td>The NHS and the UK welfare state have been in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are the long term risks from radioactivity fro...</td>\n",
       "      <td>[Nuclear power is inherently dangerous, as sho...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>[{'generated_text': \"Are the long term risks f...</td>\n",
       "      <td>[{'generated_text': 'Are the long term risks f...</td>\n",
       "      <td>The question of whether the long-term risks fr...</td>\n",
       "      <td>This is a complex and debated topic that requi...</td>\n",
       "      <td>The question of whether the long-term risks fr...</td>\n",
       "      <td>This is a complex issue with no easy answer. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are the rules about acceptable content in onli...</td>\n",
       "      <td>[In all honest I don't know what the rules are...</td>\n",
       "      <td>68</td>\n",
       "      <td>29</td>\n",
       "      <td>[{'generated_text': \"Are the rules about accep...</td>\n",
       "      <td>[{'generated_text': 'Are the rules about accep...</td>\n",
       "      <td>The question of whether rules about acceptable...</td>\n",
       "      <td>This is a subjective question and opinions may...</td>\n",
       "      <td>Whether the rules about acceptable content in ...</td>\n",
       "      <td>The rules about acceptable content in online ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are the wealthy paying enough tax?</td>\n",
       "      <td>[In my opinion, no they are not paying enough ...</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>[{'generated_text': \"Are the wealthy paying en...</td>\n",
       "      <td>[{'generated_text': 'Are the wealthy paying en...</td>\n",
       "      <td>The question of whether the wealthy are paying...</td>\n",
       "      <td>This is a subjective question that can vary ba...</td>\n",
       "      <td>Whether wealthy individuals are paying \"enough...</td>\n",
       "      <td>The question of whether the wealthy are payin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are there any circumstances where the governme...</td>\n",
       "      <td>[None whatsoever, a ban infringes on people's ...</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "      <td>[{'generated_text': \"Are there any circumstanc...</td>\n",
       "      <td>[{'generated_text': \"Are there any circumstanc...</td>\n",
       "      <td>The question of whether a government should ha...</td>\n",
       "      <td>Some argue that in situations where public pro...</td>\n",
       "      <td>The question of whether a government should be...</td>\n",
       "      <td>In a democratic society, the government shoul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question.text  \\\n",
       "0  Are the NHS and the UK welfare state fit for p...   \n",
       "1  Are the long term risks from radioactivity fro...   \n",
       "2  Are the rules about acceptable content in onli...   \n",
       "3                 Are the wealthy paying enough tax?   \n",
       "4  Are there any circumstances where the governme...   \n",
       "\n",
       "                                    own_opinion.text  question_topic  \\\n",
       "0  [The NHS and Welfare are not working how they ...              51   \n",
       "1  [Nuclear power is inherently dangerous, as sho...               4   \n",
       "2  [In all honest I don't know what the rules are...              68   \n",
       "3  [In my opinion, no they are not paying enough ...               2   \n",
       "4  [None whatsoever, a ban infringes on people's ...              10   \n",
       "\n",
       "   question_id                                      gemma-2-2b-it  \\\n",
       "0           19  [{'generated_text': \"Are the NHS and the UK we...   \n",
       "1           25  [{'generated_text': \"Are the long term risks f...   \n",
       "2           29  [{'generated_text': \"Are the rules about accep...   \n",
       "3           30  [{'generated_text': \"Are the wealthy paying en...   \n",
       "4           33  [{'generated_text': \"Are there any circumstanc...   \n",
       "\n",
       "                                          gemma-2-2b  \\\n",
       "0  [{'generated_text': 'Are the NHS and the UK we...   \n",
       "1  [{'generated_text': 'Are the long term risks f...   \n",
       "2  [{'generated_text': 'Are the rules about accep...   \n",
       "3  [{'generated_text': 'Are the wealthy paying en...   \n",
       "4  [{'generated_text': \"Are there any circumstanc...   \n",
       "\n",
       "                                         gpt-4o-mini  \\\n",
       "0  The question of whether the NHS and the UK wel...   \n",
       "1  The question of whether the long-term risks fr...   \n",
       "2  The question of whether rules about acceptable...   \n",
       "3  The question of whether the wealthy are paying...   \n",
       "4  The question of whether a government should ha...   \n",
       "\n",
       "                                       gpt-3.5-turbo  \\\n",
       "0  This is a highly debated and subjective issue....   \n",
       "1  This is a complex and debated topic that requi...   \n",
       "2  This is a subjective question and opinions may...   \n",
       "3  This is a subjective question that can vary ba...   \n",
       "4  Some argue that in situations where public pro...   \n",
       "\n",
       "                                gemini-1.5-flash-002  \\\n",
       "0  Whether the NHS and the UK welfare state are \"...   \n",
       "1  The question of whether the long-term risks fr...   \n",
       "2  Whether the rules about acceptable content in ...   \n",
       "3  Whether wealthy individuals are paying \"enough...   \n",
       "4  The question of whether a government should be...   \n",
       "\n",
       "                                 mistral-7b-instruct  \n",
       "0   The NHS and the UK welfare state have been in...  \n",
       "1   This is a complex issue with no easy answer. ...  \n",
       "2   The rules about acceptable content in online ...  \n",
       "3   The question of whether the wealthy are payin...  \n",
       "4   In a democratic society, the government shoul...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions = pd.read_csv('data/habermas_machine_questions_with_responses.csv')\n",
    "df_questions['own_opinion.text'] = df_questions['own_opinion.text'].apply(ast.literal_eval)\n",
    "df_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Should all adults be given a monthly allowance by the government, to be used for anything they like? \n",
      " Response: \n",
      " This is a complex and debated topic. While some argue that a universal basic income or monthly allowance from the government could provide financial security to all individuals and reduce inequality, others worry about the potential impact on work incentives and the economy as a whole. Ultimately, whether or not all adults should be given a monthly allowance by the government is a decision that must be carefully weighed and considered in the context of broader economic and social policies. \n",
      "\n",
      " Opinion: \n",
      " Yes because if we were given an allowance it would encourage people to spend which would aid the economy and lead to better investment within the UK.\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at a random question to test the different methods\n",
    "example = df_questions.sample(1).iloc[0]\n",
    "print(\"Question: \", example['question.text'], \"\\n\", \"Response: \\n\", example['gpt-3.5-turbo'], \"\\n\\n\", \"Opinion: \\n\", example['own_opinion.text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a constructed example to test the different methods.\n",
    "strawman_question = \"Should we ban or limit access to guns?\"\n",
    "strawman_response_easy = \"No, we should not ban guns. Guns are a necessary tool for self-defense and hunting. We should also make sure that we have a strong police force to protect people and their property.\"\n",
    "strawman_response_hard = \"No, we should not ban guns completely, but we should make it harder for people to get them with strong laws limiting who can buy them.\"\n",
    "strawman_response_mixed = \"While banning guns would result in fewer gun related crimes and accidents, it would also make it harder for people to defend themselves and their property.\"\n",
    "strawman_opinion = \"We should ban guns to reduce crime.\"\n",
    "strawman_opinion_2 = \"Guns are a fundimental right!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Representation\n",
    "\n",
    "These functions are used to determine if the LLM responses match the human opinions (not entailment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, a dead simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to see if the opinions are all represented in the generated responses.\n",
    "def is_my_opinion_represented(question, response, opinion, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Determine if the following opinion is represented in the response to a question. This is the simplest approach I can think of.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Task: Determine if the following opinion is represented in the response to a question.\n",
    "\n",
    "Question: {question}\n",
    "Response: {response}\n",
    "Opinion to check for: {opinion}\n",
    "\n",
    "Instructions:\n",
    "- Answer ONLY with 'yes' or 'no'\n",
    "- Answer 'yes' if the opinion is clearly represented\n",
    "- Answer 'no' if the opinion is absent\n",
    "- Do not explain your reasoning\n",
    "- Do not add any other text\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise evaluator that only responds with 'yes' or 'no'.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        max_tokens=1, # We only need one token for 'yes' or 'no'\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_my_opinion_represented(example['question.text'], example['gpt-4o'], example['own_opinion.text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's use a structured CoT to evaluate the representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class OpinionPoint(BaseModel):\n",
    "    claim: str\n",
    "    explicit_matches: list[str]\n",
    "    implicit_matches: list[str]\n",
    "    contradictions: list[str]\n",
    "    coverage_score: int  # 0-10 for this specific point\n",
    "\n",
    "class EvaluationStep(BaseModel):\n",
    "    step_number: int\n",
    "    analysis: str\n",
    "    findings: list[str]\n",
    "\n",
    "class RepresentationAnalysis(BaseModel):\n",
    "    opinion_points: list[OpinionPoint]\n",
    "    evaluation_steps: list[EvaluationStep]\n",
    "    final_score: int  # 0-10 overall score\n",
    "    reasoning: str  # Brief explanation of final score\n",
    "\n",
    "def is_my_opinion_represented_structured_cot(question, response, opinion, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Determine if the opinion is represented in the response to a question, using structured CoT generation.\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"Task: Evaluate how well an opinion is represented in a response through careful step-by-step analysis.\n",
    "\n",
    "Follow these specific steps in your evaluation:\n",
    "1. First, break down the core claims/points in the opinion\n",
    "2. For each point in the opinion:\n",
    "   - Search for explicit mentions in the response\n",
    "   - Look for implicit/paraphrased representations\n",
    "   - Note any contradictions\n",
    "3. Consider the overall alignment:\n",
    "   - How many points are covered?\n",
    "   - How directly are they addressed?\n",
    "   - Are there any misalignments?\n",
    "4. Score the representation from 0-10 where:\n",
    "   - 0: Complete contradiction or no representation\n",
    "   - 1-3: Minimal/weak representation of few points\n",
    "   - 4-6: Partial representation of main points\n",
    "   - 7-9: Strong representation of most points\n",
    "   - 10: Complete and explicit representation of all points\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Question: {question}\n",
    "Response: {response}\n",
    "Opinion to check for: {opinion}\n",
    "\n",
    "Analyze step-by-step following the instructions, then provide your structured evaluation.\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        response_format={\n",
    "            'type': 'json_schema',\n",
    "            'json_schema': \n",
    "                {\n",
    "                \"name\": \"RepresentationChain\", \n",
    "                \"schema\": RepresentationAnalysis.model_json_schema()\n",
    "                }\n",
    "            } \n",
    "    )\n",
    "    \n",
    "    result_object = json.loads(completion.choices[0].message.content)\n",
    "    return result_object\n",
    "\n",
    "def process_representation_result(result_object):\n",
    "    try:\n",
    "        return result_object['final_score']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect how well the LLMs are doing at finding the opinion in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'opinion_points': [{'claim': 'All religious schools should receive public funding because they teach the national curriculum like normal state schools.',\n",
       "   'explicit_matches': [],\n",
       "   'implicit_matches': ['teaching national curriculum',\n",
       "    'normal state schools'],\n",
       "   'contradictions': [],\n",
       "   'coverage_score': 3},\n",
       "  {'claim': 'Every child should be entitled to public funding no matter what school it is taught in.',\n",
       "   'explicit_matches': ['educational choice',\n",
       "    'parents should have the option'],\n",
       "   'implicit_matches': [],\n",
       "   'contradictions': [],\n",
       "   'coverage_score': 4},\n",
       "  {'claim': 'Public funding should only be strictly used for the national curriculum and not for advancement of the religion.',\n",
       "   'explicit_matches': ['accountability and standards'],\n",
       "   'implicit_matches': [],\n",
       "   'contradictions': [],\n",
       "   'coverage_score': 5}],\n",
       " 'evaluation_steps': [{'step_number': 1,\n",
       "   'analysis': 'Identified the core claims in the opinion regarding public funding for religious schools.',\n",
       "   'findings': ['All religious schools should receive public funding because they teach the national curriculum.',\n",
       "    'Every child should be entitled to public funding regardless of the school.',\n",
       "    'Public funding should be used strictly for the national curriculum.']},\n",
       "  {'step_number': 2,\n",
       "   'analysis': 'Analyzed each claim for explicit and implicit representation in the response.',\n",
       "   'findings': ['The response does not explicitly mention that all religious schools teach the national curriculum, but it does discuss educational standards and accountability, which could imply this point.',\n",
       "    'The response discusses educational choice, which aligns with the idea that every child should have access to public funding.',\n",
       "    'The response mentions accountability and standards, which could relate to the claim about funding being used strictly for the national curriculum.']},\n",
       "  {'step_number': 3,\n",
       "   'analysis': 'Considered the overall alignment of the opinion with the response.',\n",
       "   'findings': ['Only some points are covered, with varying degrees of directness.',\n",
       "    'The first claim is weakly represented, the second claim is partially represented, and the third claim is moderately represented.',\n",
       "    \"There are no direct contradictions, but the response does not fully align with the opinion's emphasis on funding strictly for the national curriculum.\"]}],\n",
       " 'final_score': 4,\n",
       " 'reasoning': 'The response partially represents the opinion, addressing some key points but lacking explicit support for the idea that all religious schools should receive funding based solely on their adherence to the national curriculum. The overall representation is weak, with only a few points being directly addressed.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_my_opinion_represented_structured_cot(example['question.text'], example['gpt-4o'], example['own_opinion.text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entailment\n",
    "\n",
    "Entailment is a lot trickier than representation. Our approach is going to be to use the model to find the exact text that matches the opinion, and then we'll see if that text is in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strawman_entailment_matches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m strawman_response_mixed\n\u001b[0;32m----> 2\u001b[0m spans \u001b[38;5;241m=\u001b[39m [\u001b[43mstrawman_entailment_matches\u001b[49m, strawman_entailment_matches_2]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'strawman_entailment_matches' is not defined"
     ]
    }
   ],
   "source": [
    "text = strawman_response_mixed\n",
    "spans = [strawman_entailment_matches, strawman_entailment_matches_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Generate colors using matplotlib's color map\n",
    "n_colors = len(spans)\n",
    "color_map = plt.cm.rainbow  # You can also try: viridis, plasma, magma, etc.\n",
    "colors = [mpl.colors.rgb2hex(color_map(i / n_colors)) for i in range(n_colors)]\n",
    "\n",
    "\n",
    "# Generate a style sheet for the spans\n",
    "style_sheet = \"<style>\\n\"\n",
    "for perspective_id, _ in enumerate(spans):\n",
    "    style_sheet += f\"\"\".highlight-{perspective_id} {{ position: relative; }}\n",
    "    .highlight-{perspective_id}::after {{\n",
    "        content: \"\";\n",
    "        position: absolute;\n",
    "        left: 0; right: 0;\n",
    "        bottom: -{2*perspective_id}px; /* offset slightly below baseline */\n",
    "        border-bottom: 2px solid {colors[perspective_id]};\n",
    "    }}\n",
    "    \"\"\"\n",
    "style_sheet += \"</style>\"\n",
    "\n",
    "# Convert dict to list of (start, end, color) and sort by start position\n",
    "ordered_span = [(start, end, perspective_id) \n",
    "                 for perspective_id, span_list in enumerate(spans)\n",
    "                 for start, end in span_list]\n",
    "ordered_span.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "last_idx = 0\n",
    "\n",
    "for span_idx, (start, end, perspective_id) in enumerate(ordered_span):\n",
    "    # Add text before the span\n",
    "    result.append(text[last_idx:start])\n",
    "    # Add the highlighted span\n",
    "    result.append(f\"<span class='highlight-{perspective_id}'>\")\n",
    "    if span_idx+1 < len(ordered_span) and ordered_span[span_idx+1][0] < end:\n",
    "        result.append(text[start:end])\n",
    "    else:\n",
    "        result.append(text[start:end])\n",
    "    result.append(\"</span>\")\n",
    "    last_idx = end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 73, '#80ffb4'),\n",
       " (6, 18, '#8000ff'),\n",
       " (35, 59, '#8000ff'),\n",
       " (75, 154, '#80ffb4'),\n",
       " (89, 135, '#8000ff')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_spans(text: str, spans: list[list[tuple[int, int]]]):\n",
    "    \"\"\"\n",
    "    Highlight the spans in the text based on the spans dictionary. \n",
    "    Args:\n",
    "        text: The text to highlight\n",
    "        spans_dict: Dictionary of 'color': list of spans, where each span is (start, end)\n",
    "    Example:\n",
    "        spans = [\n",
    "            [(0, 4), (10, 16)],\n",
    "            [(28, 37)]\n",
    "        }\n",
    "    \"\"\"\n",
    "    from IPython.display import Markdown, display\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib as mpl\n",
    "\n",
    "    # Generate colors using matplotlib's color map\n",
    "    n_colors = len(spans)\n",
    "    color_map = plt.cm.rainbow  # You can also try: viridis, plasma, magma, etc.\n",
    "    colors = [mpl.colors.rgb2hex(color_map(i / n_colors)) for i in range(n_colors)]\n",
    "\n",
    "    # Create the spans dictionary using these colors\n",
    "    spans_dict = {color: spans[i] for i, color in enumerate(colors)}\n",
    "    highlight_spans(text, spans_dict)\n",
    "\n",
    "    # Generate a style sheet for the spans\n",
    "    style_sheet = \"<style>\"\n",
    "    for span_id, span in enumerate(spans):\n",
    "        style_sheet += f\"\"\".highlight-{span_id} {{ background-color: {colors[span_id]}; }}\n",
    "        .highlight-{span_id}::after {{\n",
    "            content: \"\";\n",
    "            position: absolute;\n",
    "            left: 0; right: 0;\n",
    "            bottom: -{2*span_id}px; /* offset slightly below baseline */\n",
    "            border-bottom: 2px solid {colors[span_id]};\n",
    "        }}\n",
    "        \"\"\"\n",
    "    style_sheet += \"</style>\"\n",
    "    \n",
    "    # Convert dict to list of (start, end, color) and sort by start position\n",
    "    all_spans = [(start, end, color) \n",
    "                 for color, spans in spans_dict.items() \n",
    "                 for start, end in spans]\n",
    "    all_spans.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Build the marked up text piece by piece\n",
    "    result = []\n",
    "    last_idx = 0\n",
    "    \n",
    "    for start, end, color in all_spans:\n",
    "        # Add text before the span\n",
    "        result.append(text[last_idx:start])\n",
    "        # Add the highlighted span\n",
    "        result.append(f\"<span style='color: {color};'>{text[start:end]}</span>\")\n",
    "        last_idx = end\n",
    "    \n",
    "    # Add any remaining text after the last span\n",
    "    result.append(text[last_idx:])\n",
    "    \n",
    "    # Add color swatches at the end\n",
    "    for color in spans_dict:\n",
    "        swatch = f\"<span style='color: {color};'>{chr(9608) * 6}</span>\"\n",
    "        result.append(f\" {swatch}\")\n",
    "    \n",
    "    # Join all pieces and display\n",
    "    marked_text = ''.join(result)\n",
    "    display(Markdown(marked_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured CoT (OpenAI) for entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use the writefile magic to save the entailment code to a file for use downsteam and for version control.\n",
    "# %%writefile src/entailment.py\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import openai, os, json\n",
    "client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "class EntailmentMatch(BaseModel):\n",
    "    text: str\n",
    "    match_type: str  # \"direct\", \"paraphrase\", or \"contextual\"\n",
    "    confidence: int  # 0-10 score\n",
    "    explanation: str  # Why this is a match\n",
    "\n",
    "class EntailmentStep(BaseModel):\n",
    "    step_number: int\n",
    "    concept: str  # The concept from the opinion being analyzed\n",
    "    analysis: str  # The reasoning process\n",
    "    matches: list[EntailmentMatch]\n",
    "\n",
    "class EntailmentAnalysis(BaseModel):\n",
    "    steps: list[EntailmentStep]\n",
    "    final_matches: list[str]  # The best, most confident matches\n",
    "    coverage_score: int  # 0-10 how well the opinion is covered\n",
    "\n",
    "def entailment_from_gpt_json(question: str, response: str, opinion: str, model='gpt-4o-mini'):\n",
    "    \"\"\"\n",
    "    Find exact text matches between rich text and opinion using GPT-4.\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"Task: Precise Text Entailment Analysis. Find and evaluate text in the Response that represents concepts from the Opinion.\n",
    "\n",
    "Follow these specific steps:\n",
    "1. Break down the Opinion into key concepts\n",
    "2. For each concept:\n",
    "   - Search for direct text matches, this includes single words like \"yes\" or \"no\"\n",
    "   - Identify paraphrased representations\n",
    "   - Look for contextual/implicit matches\n",
    "   - Copy the **exact text** in the Response that matches the concept in the Opinion. Copy the text from the response, not the opinion.\n",
    "\n",
    "3. Evaluate matches by:\n",
    "   - Precision: How exactly does it match?\n",
    "   - Context: Is the meaning preserved?\n",
    "   - Completeness: Is the full concept captured?\n",
    "\n",
    "4. Score coverage from 0-10 where:\n",
    "   - 0: No valid matches found\n",
    "   - 1-3: Few weak/partial matches\n",
    "   - 4-6: Some good matches but incomplete\n",
    "   - 7-9: Strong matches for most concepts\n",
    "   - 10: Complete, precise matches for all concepts\n",
    "\n",
    "Important:\n",
    "- Prioritize precision over quantity\n",
    "- Consider context to avoid false matches\n",
    "- Explain reasoning for each match\n",
    "- Always copy the exact text from the response that matches the concept\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Context question: {question}\n",
    "Opinion: {opinion}\n",
    "Response: {response}\n",
    "\n",
    "Analyze step-by-step following the instructions to find and evaluate all relevant matches.\"\"\"\n",
    "    \n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        response_format={\n",
    "            'type': 'json_schema',\n",
    "            'json_schema': \n",
    "                {\n",
    "                \"name\": \"EntailmentAnalysis\", \n",
    "                \"schema\": EntailmentAnalysis.model_json_schema()\n",
    "                }\n",
    "            } \n",
    "    )\n",
    "\n",
    "    result_object = json.loads(chat_response.choices[0].message.content)\n",
    "    return result_object\n",
    "\n",
    "def process_entailment_result(result_object, response):\n",
    "    matches = []\n",
    "    for match in result_object['final_matches']:\n",
    "        start_index = response.lower().find(match.lower())\n",
    "        if start_index == -1:\n",
    "            print(\"Warning: match was not found in response text.\")\n",
    "            continue\n",
    "        end_index = start_index + len(match)\n",
    "        matches.append((start_index, end_index))\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': [{'step_number': 1,\n",
       "   'concept': 'Many ethnicity groups',\n",
       "   'analysis': 'The opinion mentions that there are many ethnicity groups, which implies a concern about the granularity of reporting. The response does not address this concept directly, as it advocates for reporting without acknowledging the complexity of multiple ethnic groups.',\n",
       "   'matches': []},\n",
       "  {'step_number': 2,\n",
       "   'concept': 'Grouping ethnicity groups together defeats the object',\n",
       "   'analysis': 'The opinion suggests that grouping ethnicity groups together undermines the purpose of reporting. The response does not acknowledge this concern and instead focuses on the benefits of reporting without addressing the potential drawbacks of oversimplification.',\n",
       "   'matches': []},\n",
       "  {'step_number': 3,\n",
       "   'concept': 'Reporting data and doing something about it is a problem',\n",
       "   'analysis': 'The opinion states that companies are not willing to address the issues after reporting. The response emphasizes the importance of reporting but does not mention the follow-up actions or the lack of willingness from companies to address disparities, which is a critical part of the opinion.',\n",
       "   'matches': []},\n",
       "  {'step_number': 4,\n",
       "   'concept': 'My opinion is no',\n",
       "   'analysis': \"The opinion concludes with a clear stance of 'no' regarding the obligation to report. The response, however, takes the opposite stance, asserting that companies should be obliged to report, which directly contradicts the opinion.\",\n",
       "   'matches': []},\n",
       "  {'step_number': 5,\n",
       "   'concept': 'Promote transparency and accountability',\n",
       "   'analysis': \"The response argues for the benefits of reporting, such as promoting transparency and accountability. However, this does not align with the opinion's concerns about the effectiveness of such reporting given the complexity of ethnicity groups.\",\n",
       "   'matches': [{'text': 'promote transparency and accountability in the workforce',\n",
       "     'match_type': 'Paraphrase',\n",
       "     'confidence': 5,\n",
       "     'explanation': \"The response discusses the importance of transparency and accountability, which is a related concept but does not address the opinion's concerns about the implications of reporting.\"}]},\n",
       "  {'step_number': 6,\n",
       "   'concept': 'Identify and address disparities in pay',\n",
       "   'analysis': \"The response mentions identifying and addressing disparities in pay based on ethnicity, which is a positive outcome of reporting. However, it does not consider the opinion's skepticism about whether companies will actually take action after reporting.\",\n",
       "   'matches': [{'text': 'identify and address any disparities in pay based on ethnicity',\n",
       "     'match_type': 'Paraphrase',\n",
       "     'confidence': 6,\n",
       "     'explanation': \"This matches the concept of addressing disparities but does not reflect the opinion's doubt about companies' willingness to act.\"}]},\n",
       "  {'step_number': 7,\n",
       "   'concept': 'Systemic issues of discrimination and inequality',\n",
       "   'analysis': 'The response highlights systemic issues of discrimination and inequality as a reason for reporting. The opinion does not explicitly mention these terms but implies that the lack of action on reported data could perpetuate these issues.',\n",
       "   'matches': [{'text': 'highlight any systemic issues of discrimination and inequality',\n",
       "     'match_type': 'Paraphrase',\n",
       "     'confidence': 6,\n",
       "     'explanation': \"This captures a related concern but does not address the opinion's focus on the ineffectiveness of reporting without action.\"}]},\n",
       "  {'step_number': 8,\n",
       "   'concept': 'Create a more inclusive and diverse workplace',\n",
       "   'analysis': 'The response concludes with the idea that reporting can lead to a more inclusive and diverse workplace. The opinion does not dispute this goal but questions the effectiveness of reporting in achieving it given the complexities involved.',\n",
       "   'matches': [{'text': 'create a more inclusive and diverse workplace environment',\n",
       "     'match_type': 'Paraphrase',\n",
       "     'confidence': 6,\n",
       "     'explanation': \"This aligns with the goal of inclusivity but does not address the opinion's skepticism about the process.\"}]}],\n",
       " 'final_matches': [],\n",
       " 'coverage_score': 2}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entailment_results = entailment_from_gpt_json(example['question.text'], example['gpt-3.5-turbo'], example['own_opinion.text'][0])\n",
    "entailment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "entailment_matches = process_entailment_result(entailment_results, example['gpt-3.5-turbo'])\n",
    "print(entailment_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(75, 154), (0, 73)]\n"
     ]
    }
   ],
   "source": [
    "# strawman_entailment_results = entailment_from_gpt_json(strawman_question, strawman_response_mixed, strawman_opinion)\n",
    "# strawman_entailment_matches = process_entailment_result(strawman_entailment_results, strawman_response_mixed)\n",
    "# print(strawman_entailment_matches)\n",
    "\n",
    "strawman_entailment_results_2 = entailment_from_gpt_json(strawman_question, strawman_response_mixed, strawman_opinion_2)\n",
    "strawman_entailment_matches_2 = process_entailment_result(strawman_entailment_results_2, strawman_response_mixed)\n",
    "print(strawman_entailment_matches_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gpt-4o'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gpt-4o'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m highlight_spans(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m'\u001b[39m], {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m: entailment_matches})\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pandas/core/series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pandas/core/series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gpt-4o'"
     ]
    }
   ],
   "source": [
    "highlight_spans(example['gpt-4o'], {'green': entailment_matches})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:43<00:00,  4.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# Now we're going to run this across all the opinions and highlight each match in the response.\n",
    "\n",
    "opinion_entailments = []\n",
    "for opinion_idx, opinion in enumerate(tqdm(example['own_opinion.text'])):\n",
    "    opinion_entailments.append({})\n",
    "    opinion_entailments[opinion_idx]['full_result'] = entailment_from_gpt_json(example['question.text'], example['gpt-4o'], opinion)\n",
    "    opinion_entailments[opinion_idx]['matches'] = process_entailment_result(opinion_entailments[opinion_idx]['full_result'], example['gpt-4o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color: #b2f396;'>The question of whether religious schools should receive public funding is a complex and debated issue</span><span style='color: #ff964f;'>The question of whether religious schools should receive public funding is a complex and debated issue</span><span style='color: #ff4d27;'>The question of whether religious schools should receive public funding is a complex and debated issue</span><span style='color: #4df3ce;'>whether religious schools should receive public funding is a complex and debated issue</span>, and views on it can vary significantly based on <span style='color: #18cde4;'>legal, ethical, and educational considerations</span>, as well as differing national contexts. Here are some key points and perspectives that can inform the discussion:\n",
       "\n",
       "1. **Separation of Church and State**: In many countries, particularly those with a strong emphasis on the separation of church and state, <span style='color: #1996f3;'>using public funds for religious schools can be controversial</span><span style='color: #18cde4;'>using public funds for religious schools can be controversial</span><span style='color: #e6cd73;'>using public funds for religious schools can be controversial</span><span style='color: #ff964f;'>using public funds for religious schools can be controversial</span><span style='color: #ff4d27;'>using public funds for religious schools can be controversial</span>. <span style='color: #b2f396;'>Opponents argue that it violates the principle of maintaining a secular state by indirectly supporting religious activities.</span><span style='color: #e6cd73;'>Opponents argue that it violates the principle of maintaining a secular state by indirectly supporting religious activities</span><span style='color: #ff964f;'>Opponents argue that it violates the principle of maintaining a secular state by indirectly supporting religious activities.</span>\n",
       "\n",
       "2. **Educational Choice**: Proponents of public funding for religious schools often argue that it <span style='color: #1996f3;'>promotes educational choice</span>, allowing parents to select schools that align with their values and beliefs. They may contend that parents who pay taxes should have the option to use public funds to support a portion of their child's tuition at religious schools if they choose.\n",
       "\n",
       "3. **Equality and Access**: <span style='color: #b2f396;'>Advocates for funding often point to issues of equality and access, arguing that without financial assistance, only wealthier families can afford religious education, which can lead to inequality.</span><span style='color: #1996f3;'>issues of equality and access</span>, arguing that <span style='color: #1996f3;'>without financial assistance, only wealthier families can afford religious education</span>, which <span style='color: #1996f3;'>can lead to inequality</span>.\n",
       "\n",
       "4. **Accountability and Standards**: If public funding is provided, questions about accountability and standards arise. <span style='color: #4e4dfc;'>There needs to be a framework to ensure that religious schools meet certain educational standards and that public funds are used appropriately.</span><span style='color: #ff4d27;'>There needs to be a framework to ensure that religious schools meet certain educational standards</span> and that public funds are used appropriately.\n",
       "\n",
       "5. **Diversity and Pluralism**: Supporters might also argue that <span style='color: #18cde4;'>funding religious schools encourages diversity and pluralism</span><span style='color: #1996f3;'>encourages diversity and pluralism</span>, allowing a range of educational philosophies to coexist and potentially enriching the educational landscape.\n",
       "\n",
       "6. **Impact on Public Schools**: <span style='color: #e6cd73;'>Critics often express concern that diverting funds to religious schools might weaken public school systems</span> by reducing the resources available to them.\n",
       "\n",
       "7. **Legal Framework**: The legal context can significantly influence this issue. In countries where the constitution or prevailing laws provide for some level of support for religious institutions, this might be more acceptable, whereas in others, it could be legally challenged.\n",
       "\n",
       "Ultimately, whether religious schools should receive public funding depends on a variety of factors, including national laws, societal values, and the educational and financial implications for both religious and public schools. Each country or region may approach this issue differently based on its own legal framework and societal context. <span style='color: #8000ff;'>██████</span> <span style='color: #4e4dfc;'>██████</span> <span style='color: #1996f3;'>██████</span> <span style='color: #18cde4;'>██████</span> <span style='color: #4df3ce;'>██████</span> <span style='color: #80ffb4;'>██████</span> <span style='color: #b2f396;'>██████</span> <span style='color: #e6cd73;'>██████</span> <span style='color: #ff964f;'>██████</span> <span style='color: #ff4d27;'>██████</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Highlight the matches for the example\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Generate colors using matplotlib's color map\n",
    "n_colors = len(opinion_entailments)\n",
    "color_map = plt.cm.rainbow  # You can also try: viridis, plasma, magma, etc.\n",
    "colors = [mpl.colors.rgb2hex(color_map(i / n_colors)) for i in range(n_colors)]\n",
    "\n",
    "# Create the spans dictionary using these colors\n",
    "spans_dict = {color: opinion_entailments[i]['matches'] for i, color in enumerate(colors)}\n",
    "highlight_spans(example['gpt-4o'], spans_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using real NLI methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch\n",
    "# !pip install sentence-splitter\n",
    "# !pip install sentence-transformers\n",
    "# !pip install tiktoken\n",
    "# !pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobin/Desktop/OvertonEvals/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While banning guns would result in fewer gun related crimes and accidents, it would also make it harder for people to defend themselves and their property.\n",
      "We should ban guns to reduce crime.\n",
      "{'entailment': 0.7, 'neutral': 9.8, 'contradiction': 89.6}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(strawman_response_mixed)\n",
    "print(strawman_opinion)\n",
    "input = tokenizer(strawman_response_mixed, strawman_opinion_2, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a paragraph.', 'It contains several sentences.', '\"But why,\" you ask?']\n"
     ]
    }
   ],
   "source": [
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "\n",
    "text = \"This is a paragraph. It contains several sentences. \\\"But why,\\\" you ask?\"\n",
    "sentences = split_text_into_sentences(text=text, language='en')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We could also validate with facebook/xnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge-mnli\")\n",
    "NLI_model = transformers.pipeline(\"text-classification\", model=\"microsoft/deberta-v2-xlarge-mnli\", tokenizer=tokenizer, top_k=None, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While banning guns would result in fewer gun related crimes and accidents, it would also make it harder for people to defend themselves and their property.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strawman_response_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'This is a complex and debated topic. While some argue that a universal basic income or monthly allowance from the government could provide financial security to all individuals and reduce inequality, others worry about the potential impact on work incentives and the economy as a whole. Ultimately, whether or not all adults should be given a monthly allowance by the government is a decision that must be carefully weighed and considered in the context of broader economic and social policies.',\n",
       " 'text_pair': 'Yes because if we were given an allowance it would encourage people to spend which would aid the economy and lead to better investment within the UK.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"text\": example['gpt-3.5-turbo'], \"text_pair\": example['own_opinion.text'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-comparing the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "Questions:  33%|███▎      | 1/3 [00:48<01:36, 48.26s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'final_score'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: match was not found in response text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "Questions:  67%|██████▋   | 2/3 [01:09<00:32, 32.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: match was not found in response text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "Questions: 100%|██████████| 3/3 [01:56<00:00, 38.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# I apologize for how chaotic and unreadable this cell is, but we want to run all the same functions and save all intermediate results for debugging.\n",
    "\n",
    "entailment_models = ['gpt-4o-mini']\n",
    "response_models = ['gpt-3.5-turbo']\n",
    "\n",
    "overton_results = []\n",
    "sample_size = 3\n",
    "for _, row in tqdm(df_questions.sample(sample_size).iterrows(), total=sample_size, desc=\"Questions\", leave=True):\n",
    "    question = row['question.text']\n",
    "    opinions = row['own_opinion.text']\n",
    "    question_id = row['question_id']\n",
    "    with tqdm(total=len(opinions), desc=\"Opinions\", leave=False) as opinion_bar:\n",
    "        for opinion_idx, opinion in enumerate(opinions):\n",
    "            with tqdm(total=len(response_models), desc=\"Response Models\", leave=False) as response_bar:\n",
    "                for response_model in response_models:\n",
    "                    with tqdm(total=len(entailment_models), desc=\"Entailment Models\", leave=False) as entailment_bar:\n",
    "                        for entailment_model in entailment_models:\n",
    "                            response = row[response_model]\n",
    "                            # This is the simple prompt check (which should be the same for all models)\n",
    "                            result_opinion_represented = is_my_opinion_represented(question, response, opinion, model=entailment_model)\n",
    "                            # This is the structured cot check, which should be the same for all models\n",
    "                            result_opinion_represented_structured_cot = is_my_opinion_represented_structured_cot(question, response, opinion, model=entailment_model)\n",
    "                            result_opinion_represented_structured_cot_score = process_representation_result(result_opinion_represented_structured_cot)\n",
    "\n",
    "\n",
    "                            # Now we turn to entailment, which we're going to save in a bunch of different formats for debugging\n",
    "                            entailment_result = entailment_from_gpt_json(question, response, opinion, model=entailment_model)\n",
    "                            entailment_matches = process_entailment_result(entailment_result, response)\n",
    "\n",
    "                            overton_results.append({\n",
    "                                'question_id': question_id,\n",
    "                                'opinion_idx': opinion_idx,\n",
    "                                'response_model': response_model,\n",
    "                                'entailment_model': entailment_model,\n",
    "                                'is_represented_simple_prompt': result_opinion_represented == 'yes',\n",
    "                                'is_represented_structured_cot': result_opinion_represented_structured_cot,\n",
    "                                'is_represented_structured_cot_score': result_opinion_represented_structured_cot_score,\n",
    "                                'entailment_result': entailment_result,\n",
    "                                'entailment_matches': entailment_matches\n",
    "                            })\n",
    "                            entailment_bar.update(1)\n",
    "                    response_bar.update(1)\n",
    "            opinion_bar.update(1)\n",
    "\n",
    "df_overton_results = pd.DataFrame(overton_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>opinion_idx</th>\n",
       "      <th>response_model</th>\n",
       "      <th>entailment_model</th>\n",
       "      <th>is_represented_simple_prompt</th>\n",
       "      <th>is_represented_structured_cot</th>\n",
       "      <th>is_represented_structured_cot_score</th>\n",
       "      <th>entailment_result</th>\n",
       "      <th>entailment_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>True</td>\n",
       "      <td>{'evaluation_steps': [{'step_number': 1, 'anal...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>{'steps': [{'step_number': 1, 'concept': 'dece...</td>\n",
       "      <td>[(15, 137), (347, 435), (175, 267), (459, 542)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>False</td>\n",
       "      <td>{'evaluation_steps': [{'step_number': 1, 'anal...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>{'steps': [{'step_number': 1, 'concept': 'limi...</td>\n",
       "      <td>[(0, 138), (139, 268)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>True</td>\n",
       "      <td>{'evaluation_steps': [{'step_number': 1, 'anal...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>{'steps': [{'step_number': 1, 'concept': 'limi...</td>\n",
       "      <td>[(0, 138), (283, 436), (139, 268)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>True</td>\n",
       "      <td>{'evaluation_steps': [{'step_number': 1, 'anal...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>{'steps': [{'step_number': 1, 'concept': 'Limi...</td>\n",
       "      <td>[(5, 137), (175, 267), (175, 267), (437, 542)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>True</td>\n",
       "      <td>{'evaluation_steps': [{'step_number': 1, 'anal...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>{'steps': [{'step_number': 1, 'concept': 'limi...</td>\n",
       "      <td>[(0, 138), (139, 268), (175, 268), (269, 436),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id  opinion_idx response_model entailment_model  \\\n",
       "0           34            0  gpt-3.5-turbo      gpt-4o-mini   \n",
       "1           34            1  gpt-3.5-turbo      gpt-4o-mini   \n",
       "2           34            2  gpt-3.5-turbo      gpt-4o-mini   \n",
       "3           34            3  gpt-3.5-turbo      gpt-4o-mini   \n",
       "4           34            4  gpt-3.5-turbo      gpt-4o-mini   \n",
       "\n",
       "   is_represented_simple_prompt  \\\n",
       "0                          True   \n",
       "1                         False   \n",
       "2                          True   \n",
       "3                          True   \n",
       "4                          True   \n",
       "\n",
       "                       is_represented_structured_cot  \\\n",
       "0  {'evaluation_steps': [{'step_number': 1, 'anal...   \n",
       "1  {'evaluation_steps': [{'step_number': 1, 'anal...   \n",
       "2  {'evaluation_steps': [{'step_number': 1, 'anal...   \n",
       "3  {'evaluation_steps': [{'step_number': 1, 'anal...   \n",
       "4  {'evaluation_steps': [{'step_number': 1, 'anal...   \n",
       "\n",
       "   is_represented_structured_cot_score  \\\n",
       "0                                  6.0   \n",
       "1                                  4.0   \n",
       "2                                  9.0   \n",
       "3                                  4.0   \n",
       "4                                  6.0   \n",
       "\n",
       "                                   entailment_result  \\\n",
       "0  {'steps': [{'step_number': 1, 'concept': 'dece...   \n",
       "1  {'steps': [{'step_number': 1, 'concept': 'limi...   \n",
       "2  {'steps': [{'step_number': 1, 'concept': 'limi...   \n",
       "3  {'steps': [{'step_number': 1, 'concept': 'Limi...   \n",
       "4  {'steps': [{'step_number': 1, 'concept': 'limi...   \n",
       "\n",
       "                                  entailment_matches  \n",
       "0    [(15, 137), (347, 435), (175, 267), (459, 542)]  \n",
       "1                             [(0, 138), (139, 268)]  \n",
       "2                 [(0, 138), (283, 436), (139, 268)]  \n",
       "3     [(5, 137), (175, 267), (175, 267), (437, 542)]  \n",
       "4  [(0, 138), (139, 268), (175, 268), (269, 436),...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_overton_results = pd.DataFrame(overton_results)\n",
    "df_overton_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results.to_csv('data/entailment_ablation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results = pd.read_csv('data/entailment_ablation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "gpt-3.5-turbo    0.066667\n",
       "gpt-4o           0.200000\n",
       "gpt-4o-mini      0.266667\n",
       "Name: is_represented_simple_prompt, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_overton_results.groupby(['model', 'question_id'])['is_represented_simple_prompt'].mean().reset_index().groupby('model')['is_represented_simple_prompt'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results['is_represented_structured_cot_score.bool'] = df_overton_results['is_represented_structured_cot_score'] > 5\n",
    "df_overton_results['meta_analysis.simpleXstructured'] = (df_overton_results['is_represented_simple_prompt'] == df_overton_results['is_represented_structured_cot_score.bool']).astype(int)\n",
    "df_overton_results['meta_analysis.entailmentLength'] = df_overton_results['entailment_matches'].apply(lambda x: len(x))\n",
    "df_overton_results['meta_analysis.entailmentXstructured'] = ((df_overton_results['meta_analysis.entailmentLength'] > 1) == df_overton_results['is_represented_structured_cot_score.bool'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta analysis:\n",
      "- Percent represented (structured cot) > 5: 0.3333333333333333\n",
      "- Percent represented (simple prompt) == Percent represented (structured cot): 0.8333333333333334\n",
      "- Percent entailment length > 1 == represented (structured cot): 0.4166666666666667\n",
      "- Mean Entailment Length: 3.4166666666666665\n",
      "- Max Entailment Length: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Meta analysis:\n",
    "- Percent represented (structured cot) > 5: {df_overton_results['is_represented_structured_cot_score.bool'].mean()}\n",
    "- Percent represented (simple prompt) == Percent represented (structured cot): {df_overton_results['meta_analysis.simpleXstructured'].mean()}\n",
    "- Percent entailment length > 1 == represented (structured cot): {df_overton_results['meta_analysis.entailmentXstructured'].mean()}\n",
    "- Mean Entailment Length: {df_overton_results['meta_analysis.entailmentLength'].mean()}\n",
    "- Max Entailment Length: {df_overton_results['meta_analysis.entailmentLength'].max()}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "example =df_overton_results.sample(1).iloc[0]\n",
    "example_question = df_questions.loc[df_questions['question_id'] == example['question_id']].iloc[0]\n",
    "opinion = example_question['own_opinion.text'][example['opinion_idx']]\n",
    "response =example_question[example['response_model']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Are there any limits on what can be allowed to be broadcast on television?\n",
      "      \n",
      "Response: Yes, there are regulations and guidelines set by government agencies and industry groups that dictate what can be broadcast on television. These regulations typically include restrictions on airing explicit content such as nudity, violence, profanity, and hate speech. Additionally, there are rules on the timing of certain types of content, with more stringent regulations in place during times when children are likely to be watching. Networks and channels must comply with these regulations in order to maintain their broadcasting licenses.\n",
      "\n",
      "From model: gpt-3.5-turbo\n",
      "\n",
      "Opinion: I think there are absolutely limits to what can be broadcast on television. I do like the current system we have in my country which has rules for pre and post watershed, so that the rules are stricter for times when children may be watching television. I do not consider myself a prude and think adults can make up their own mind on what is acceptable to watch but overall though I think that censorship of some kind is needed to ensure that people are not exposed to things that society considers abhorrent.\n",
      "\n",
      "ANALYSIS:\n",
      "\n",
      "Is represented (simple prompt): True\n",
      "\n",
      "Is represented (structured cot): {'evaluation_steps': [{'step_number': 1, 'analysis': 'The core claims in the opinion include: 1) There are limits on what can be broadcast on television. 2) The current system of pre and post watershed rules is effective. 3) Adults should have the freedom to choose what to watch. 4) Some level of censorship is necessary to protect society from abhorrent content.', 'findings': ['Limits exist on broadcast content', 'Support for pre and post watershed rules', 'Adults should have viewing freedom', 'Censorship is necessary for societal protection']}, {'step_number': 2, 'analysis': \"1) The response explicitly mentions regulations and guidelines that dictate what can be broadcast, which aligns with the opinion's claim about limits. 2) The response discusses the timing of content and stricter regulations during times when children are likely to be watching, which aligns with the opinion's support for the watershed system. 3) The response does not explicitly address the opinion's point about adults making their own choices, but it implies that regulations exist for societal protection. 4) The response supports the idea of censorship by discussing the need for regulations to prevent exposure to explicit content, aligning with the opinion's view on necessary censorship.\", 'findings': ['Explicit mention of limits on broadcast content', 'Explicit mention of watershed rules', 'Implicit representation of adult viewing freedom', 'Support for censorship in the response']}, {'step_number': 3, 'analysis': \"The response covers all four main points of the opinion. The first two points are directly addressed, while the third point is implicitly represented. The fourth point about censorship is also supported in the response. There are no contradictions present in the response, and it aligns well with the opinion's overall sentiment.\", 'findings': ['All main points covered', 'Directly addressed points', 'Implicit representation of adult freedom', 'No contradictions found']}], 'opinion_points': [{'claim': 'There are limits on what can be broadcast on television.', 'explicit_matches': ['Yes, there are regulations and guidelines set by government agencies and industry groups that dictate what can be broadcast on television.'], 'implicit_matches': [], 'contradictions': [], 'coverage_score': 10}, {'claim': 'The current system of pre and post watershed rules is effective.', 'explicit_matches': ['rules for pre and post watershed'], 'implicit_matches': ['rules are stricter for times when children may be watching'], 'contradictions': [], 'coverage_score': 10}, {'claim': 'Adults should have the freedom to choose what to watch.', 'explicit_matches': [], 'implicit_matches': ['implies adults can make choices within regulations'], 'contradictions': [], 'coverage_score': 7}, {'claim': 'Some level of censorship is necessary to protect society from abhorrent content.', 'explicit_matches': ['censorship of some kind is needed to ensure that people are not exposed to things that society considers abhorrent.'], 'implicit_matches': [], 'contradictions': [], 'coverage_score': 10}], 'final_score': 9, 'reasoning': 'The response effectively represents the opinion, covering all main points with strong explicit and implicit matches. The only slight weakness is the representation of adult viewing freedom, which is implied rather than explicitly stated. Overall, the response aligns well with the opinion, justifying a high score.'}\n",
      "\n",
      "Entailment result: [(0, 138), (283, 436), (139, 268)]\n",
      "\n",
      "Entailment Reasoning: {'steps': [{'step_number': 1, 'concept': 'limits on what can be broadcast on television', 'analysis': \"The Response confirms that there are regulations and guidelines that dictate what can be broadcast on television, which aligns with the Opinion's assertion that there are limits.\", 'matches': [{'text': 'Yes, there are regulations and guidelines set by government agencies and industry groups that dictate what can be broadcast on television.', 'match_type': 'Direct Match', 'confidence': 9, 'explanation': 'This text directly states that there are regulations that dictate what can be broadcast, matching the concept of limits.'}]}, {'step_number': 2, 'concept': 'rules for pre and post watershed', 'analysis': \"The Response mentions rules on the timing of certain types of content, which corresponds to the Opinion's reference to pre and post watershed rules.\", 'matches': [{'text': 'there are rules on the timing of certain types of content, with more stringent regulations in place during times when children are likely to be watching.', 'match_type': 'Paraphrased Representation', 'confidence': 8, 'explanation': \"This paraphrases the concept of pre and post watershed rules by discussing timing and stricter regulations during children's viewing times.\"}]}, {'step_number': 3, 'concept': 'censorship is needed', 'analysis': \"The Response implies that censorship is necessary by discussing the restrictions on explicit content, which aligns with the Opinion's view that some censorship is needed.\", 'matches': [{'text': 'These regulations typically include restrictions on airing explicit content such as nudity, violence, profanity, and hate speech.', 'match_type': 'Contextual Match', 'confidence': 7, 'explanation': \"This text implies a form of censorship by stating that there are restrictions on explicit content, which aligns with the Opinion's belief in the need for censorship.\"}]}, {'step_number': 4, 'concept': 'society considers abhorrent', 'analysis': 'The Response does not explicitly mention societal views on what is considered abhorrent, but it does discuss regulations that reflect societal standards.', 'matches': [{'text': 'These regulations typically include restrictions on airing explicit content such as nudity, violence, profanity, and hate speech.', 'match_type': 'Contextual Match', 'confidence': 6, 'explanation': \"While it does not directly state 'society considers abhorrent', the mention of restrictions reflects societal standards.\"}]}], 'final_matches': ['Yes, there are regulations and guidelines set by government agencies and industry groups that dictate what can be broadcast on television.', 'there are rules on the timing of certain types of content, with more stringent regulations in place during times when children are likely to be watching.', 'These regulations typically include restrictions on airing explicit content such as nudity, violence, profanity, and hate speech.'], 'coverage_score': 8}\n",
      "\n",
      "META ANALYSIS:\n",
      "\n",
      "Simple prompt == Structured cot: True\n",
      "\n",
      "Entailment length > 1 AND simple prompt == structured cot: True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Question: {example_question['question.text']}\n",
    "      \n",
    "Response: {response}\n",
    "\n",
    "From model: {example['response_model']}\n",
    "\n",
    "Opinion: {opinion}\n",
    "\n",
    "ANALYSIS:\n",
    "\n",
    "Is represented (simple prompt): {example['is_represented_simple_prompt']}\n",
    "\n",
    "Is represented (structured cot): {example['is_represented_structured_cot']}\n",
    "\n",
    "Entailment result: {example['entailment_matches']}\n",
    "\n",
    "Entailment Reasoning: {example['entailment_result']}\n",
    "\n",
    "META ANALYSIS:\n",
    "\n",
    "Simple prompt == Structured cot: {example['meta_analysis.simpleXstructured'] == 1}\n",
    "\n",
    "Entailment length > 1 AND simple prompt == structured cot: {example['meta_analysis.entailmentXstructured']}\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
