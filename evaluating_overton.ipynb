{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, we will evaluate the responses generated by the LLMs.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question.text</th>\n",
       "      <th>own_opinion.text</th>\n",
       "      <th>question_topic</th>\n",
       "      <th>question_id</th>\n",
       "      <th>gpt-4o-mini</th>\n",
       "      <th>gpt-3.5-turbo</th>\n",
       "      <th>gpt-4o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are laws banning private education a good idea?</td>\n",
       "      <td>[I am indifferent regarding this statement for...</td>\n",
       "      <td>70</td>\n",
       "      <td>7</td>\n",
       "      <td>The debate over whether laws banning private e...</td>\n",
       "      <td>Banning private education completely may not b...</td>\n",
       "      <td>The question of whether laws banning private e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If we could extend human life by 50 years, sho...</td>\n",
       "      <td>[I think we should always advance science and ...</td>\n",
       "      <td>90</td>\n",
       "      <td>128</td>\n",
       "      <td>The question of whether we should extend human...</td>\n",
       "      <td>This question is highly subjective and there a...</td>\n",
       "      <td>Extending human life by 50 years is a complex ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is it OK to use animal products at all for bea...</td>\n",
       "      <td>[I do not think it is ok to use animal product...</td>\n",
       "      <td>5</td>\n",
       "      <td>157</td>\n",
       "      <td>The use of animal products in beauty care is a...</td>\n",
       "      <td>This is a personal choice and ultimately depen...</td>\n",
       "      <td>The use of animal products in beauty care is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is the use of fossil fuels doing more harm tha...</td>\n",
       "      <td>[This question is if you believe in man- made ...</td>\n",
       "      <td>39</td>\n",
       "      <td>343</td>\n",
       "      <td>The impact of fossil fuels on the economy is a...</td>\n",
       "      <td>The use of fossil fuels has both positive and ...</td>\n",
       "      <td>The use of fossil fuels is a complex issue wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Should parents have a legal right to override ...</td>\n",
       "      <td>[Parents should have a legal right to some deg...</td>\n",
       "      <td>61</td>\n",
       "      <td>511</td>\n",
       "      <td>The question of whether parents should have a ...</td>\n",
       "      <td>There is no simple answer to this question as ...</td>\n",
       "      <td>The question of whether parents should have a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question.text  \\\n",
       "0    Are laws banning private education a good idea?   \n",
       "1  If we could extend human life by 50 years, sho...   \n",
       "2  Is it OK to use animal products at all for bea...   \n",
       "3  Is the use of fossil fuels doing more harm tha...   \n",
       "4  Should parents have a legal right to override ...   \n",
       "\n",
       "                                    own_opinion.text  question_topic  \\\n",
       "0  [I am indifferent regarding this statement for...              70   \n",
       "1  [I think we should always advance science and ...              90   \n",
       "2  [I do not think it is ok to use animal product...               5   \n",
       "3  [This question is if you believe in man- made ...              39   \n",
       "4  [Parents should have a legal right to some deg...              61   \n",
       "\n",
       "   question_id                                        gpt-4o-mini  \\\n",
       "0            7  The debate over whether laws banning private e...   \n",
       "1          128  The question of whether we should extend human...   \n",
       "2          157  The use of animal products in beauty care is a...   \n",
       "3          343  The impact of fossil fuels on the economy is a...   \n",
       "4          511  The question of whether parents should have a ...   \n",
       "\n",
       "                                       gpt-3.5-turbo  \\\n",
       "0  Banning private education completely may not b...   \n",
       "1  This question is highly subjective and there a...   \n",
       "2  This is a personal choice and ultimately depen...   \n",
       "3  The use of fossil fuels has both positive and ...   \n",
       "4  There is no simple answer to this question as ...   \n",
       "\n",
       "                                              gpt-4o  \n",
       "0  The question of whether laws banning private e...  \n",
       "1  Extending human life by 50 years is a complex ...  \n",
       "2  The use of animal products in beauty care is a...  \n",
       "3  The use of fossil fuels is a complex issue wit...  \n",
       "4  The question of whether parents should have a ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions = pd.read_csv('data/habermas_machine_questions_with_responses.csv')\n",
    "df_questions['own_opinion.text'] = df_questions['own_opinion.text'].apply(ast.literal_eval)\n",
    "df_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to see if the opinions are all represented in the generated responses.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def is_my_opinion_represented(question, response, opinion):\n",
    "    \"\"\"\n",
    "    Determine if the following opinion is represented in the response to a question. This is the simplest approach I can think of.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Task: Determine if the following opinion is represented in the response to a question.\n",
    "\n",
    "Question: {question}\n",
    "Response: {response}\n",
    "Opinion to check for: {opinion}\n",
    "\n",
    "Instructions:\n",
    "- Answer ONLY with 'yes' or 'no'\n",
    "- Answer 'yes' if the opinion is clearly represented\n",
    "- Answer 'no' if the opinion is absent\n",
    "- Do not explain your reasoning\n",
    "- Do not add any other text\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise evaluator that only responds with 'yes' or 'no'.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        max_tokens=1, # We only need one token for 'yes' or 'no'\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_my_opinion_represented_with_cot_michiel(question, response, opinion):\n",
    "    \"\"\"\n",
    "    Determine if the following opinion is represented in the response to a question. Using a CoT with XML formatting (task for Michiel Bakker).\n",
    "    \"\"\"\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class ThinkingStep(BaseModel):\n",
    "    step_number: int\n",
    "    thought: str\n",
    "\n",
    "class RepresentationChain(BaseModel):\n",
    "    steps: list[ThinkingStep]\n",
    "    relevance_score: int\n",
    "\n",
    "def is_my_opinion_represented_structured_cot(question, response, opinion):\n",
    "    \"\"\"\n",
    "    Determine if the opinion is represented in the response to a question, using structured CoT generation.\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"Task: Determine if the opinion is represented in the response to the question. \n",
    "You should think step-by-step about the question and response to determine if the opinion is represented and then give a score between 0 and 10 of how well it is represented.\n",
    "\"\"\"\n",
    "    \n",
    "    prompt =\"\"\"Question: {question}\n",
    "Response: {response}\n",
    "Opinion to check for: {opinion}\n",
    "\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise evaluator that only responds with 'yes' or 'no'.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        response_format={\n",
    "            'type': 'json_schema',\n",
    "            'json_schema': \n",
    "                {\n",
    "                \"name\": \"RepresentationChain\", \n",
    "                \"schema\": RepresentationChain.model_json_schema()\n",
    "                }\n",
    "            } \n",
    "    )\n",
    "    \n",
    "    result_object = json.loads(completion.choices[0].message.content)\n",
    "    return result_object\n",
    "\n",
    "def process_representation_result(result_object):\n",
    "    return result_object['relevance_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class ThinkingStep(BaseModel):\n",
    "    step_number: int\n",
    "    thought: str\n",
    "\n",
    "class EntailmentJSON(BaseModel):\n",
    "    matches: list[str]\n",
    "\n",
    "class EntailmentChain(BaseModel):\n",
    "    steps: list[ThinkingStep]\n",
    "    final_answer: EntailmentJSON\n",
    "\n",
    "def entailment_from_gpt_json(question: str, response: str, opinion: str):\n",
    "    \"\"\"\n",
    "    Find exact text matches between rich text and opinion using GPT-4.\n",
    "\n",
    "    Args:\n",
    "        question: Context question or query\n",
    "        response: Source text to find matches in\n",
    "        opinion: Opinion text to match against\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing matches as an array of strings\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"Task: Text Entailment. Find the exact words in the Rich Text question response that specifically represent the content expressed in the Opinion.\n",
    "\n",
    "Instructions:\n",
    "1. For each concept in the Opinion, find the sentences in the Rich Text that specifically represent the content expressed in the Opinion. Think through this step-by-step.\n",
    "2. Return the EXACT text from the Rich Text that represents the content expressed in the Opinion.\n",
    "\n",
    "Important:\n",
    "- Do not paraphrase or interpret - use the exact text.\n",
    "- Each match must be a continuous span of text.\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Context question: {question}\n",
    "Rich Text: {response}\n",
    "Opinion: {opinion}\n",
    "\n",
    "Find the text that is entailed by the opinion.\"\"\"\n",
    "\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,  # Use 0 for maximum consistency\n",
    "        response_format={\n",
    "            'type': 'json_schema',\n",
    "            'json_schema': \n",
    "                {\n",
    "                \"name\": \"EntailmentChain\", \n",
    "                \"schema\": EntailmentChain.model_json_schema()\n",
    "                }\n",
    "            } \n",
    "    )\n",
    "\n",
    "    result_object = json.loads(chat_response.choices[0].message.content)\n",
    "    return result_object\n",
    "\n",
    "def process_entailment_result(result_object, response):\n",
    "    matches = []\n",
    "    for match in result_object['final_answer']['matches']:\n",
    "        start_index = response.lower().find(match.lower())\n",
    "        end_index = start_index + len(match)\n",
    "        matches.append((start_index, end_index))\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-4o-mini', 'gpt-3.5-turbo','gpt-4o']\n",
    "\n",
    "overton_results = []\n",
    "for _, row in tqdm(df_questions.iterrows(), total=df_questions.shape[0]):\n",
    "    question = row['question.text']\n",
    "    opinions = row['own_opinion.text']\n",
    "    question_id = row['question_id']\n",
    "    for opinion_idx, opinion in enumerate(opinions):\n",
    "        for model in models:\n",
    "            response = row[model]\n",
    "            result_opinion_represented = is_my_opinion_represented(question, response, opinion)\n",
    "            result_structured_cot = is_my_opinion_represented_structured_cot(question, response, opinion)\n",
    "            entailment_result = entailment_from_gpt_json(question, response, opinion)\n",
    "            entailment_matches = process_entailment_result(entailment_result, response)\n",
    "\n",
    "            overton_results.append({\n",
    "                'question_id': question_id,\n",
    "                'opinion_idx': opinion_idx,\n",
    "                'model': model,\n",
    "                'is_represented_simple_prompt': result_opinion_represented == 'yes',\n",
    "                'is_represented_structured_cot': result_structured_cot,\n",
    "                'is_represented_structured_cot_score': process_representation_result(result_structured_cot),\n",
    "                'entailment_result': entailment_result,\n",
    "                'entailment_matches': entailment_matches\n",
    "            })\n",
    "\n",
    "df_overton_results = pd.DataFrame(overton_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results = pd.DataFrame(overton_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results.to_csv('data/overton_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "gpt-3.5-turbo    0.066667\n",
       "gpt-4o           0.200000\n",
       "gpt-4o-mini      0.266667\n",
       "Name: is_represented_simple_prompt, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_overton_results.groupby(['model', 'question_id'])['is_represented_simple_prompt'].mean().reset_index().groupby('model')['is_represented_simple_prompt'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results = pd.read_csv('data/overton_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overton_results['is_represented_structured_cot_score.bool'] = df_overton_results['is_represented_structured_cot_score'] > 5\n",
    "df_overton_results['meta_analysis.simpleXstructured'] = (df_overton_results['is_represented_simple_prompt'] == df_overton_results['is_represented_structured_cot_score.bool']).astype(int)\n",
    "df_overton_results['meta_analysis.entailmentLength'] = df_overton_results['entailment_matches'].apply(lambda x: len(x))\n",
    "df_overton_results['meta_analysis.entailmentXstructured'] = ((df_overton_results['meta_analysis.entailmentLength'] > 1) == df_overton_results['is_represented_structured_cot_score.bool'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta analysis:\n",
      "- Percent represented (structured cot) > 5: 0.4\n",
      "- Percent represented (simple prompt) == Percent represented (structured cot): 0.7142857142857143\n",
      "- Percent entailment length > 1 == represented (structured cot): 0.4\n",
      "- Mean Entailment Length: 24.97142857142857\n",
      "- Max Entailment Length: 44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Meta analysis:\n",
    "- Percent represented (structured cot) > 5: {df_overton_results['is_represented_structured_cot_score.bool'].mean()}\n",
    "- Percent represented (simple prompt) == Percent represented (structured cot): {df_overton_results['meta_analysis.simpleXstructured'].mean()}\n",
    "- Percent entailment length > 1 == represented (structured cot): {df_overton_results['meta_analysis.entailmentXstructured'].mean()}\n",
    "- Mean Entailment Length: {df_overton_results['meta_analysis.entailmentLength'].mean()}\n",
    "- Max Entailment Length: {df_overton_results['meta_analysis.entailmentLength'].max()}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "example =df_overton_results.sample(1).iloc[0]\n",
    "example_question = df_questions.loc[df_questions['question_id'] == example['question_id']].iloc[0]\n",
    "opinion = example_question['own_opinion.text'][example['opinion_idx']]\n",
    "response =example_question[example['model']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If we could extend human life by 50 years, should we do it?\n",
      "      \n",
      "Response: This question is highly subjective and there are various ethical, social, and scientific considerations to take into account when discussing the idea of extending human life by 50 years. Some arguments in favor of extending human life may include advancements in medical technology and the potential for individuals to experience more of their lives, achieve personal goals, and contribute to society. On the other hand, concerns may be raised about overpopulation, resource depletion, inequality in access to life-extending technologies, and the potential negative impacts on the environment.\n",
      "\n",
      "Ultimately, the decision to extend human life by 50 years would require careful consideration of these factors and a thorough examination of the possible consequences. It is important to engage in open and transparent discussions with experts, policymakers, ethicists, and members of the public to ensure that any decisions made are in the best interest of society as a whole.\n",
      "\n",
      "From model: gpt-3.5-turbo\n",
      "\n",
      "Opinion: This may seem like an easy question to answer bit I don't think it is. It would depend upon the quality of the life you would have if it was extended. It may be that it is of poor quality and with illness and disease and thus a great deal of medical care. Extending life for everyone would also have massive social consequences which would require a great deal of analysis in order to ascertain the real consequences.\n",
      "\n",
      "ANALYSIS:\n",
      "\n",
      "Is represented (simple prompt): True\n",
      "\n",
      "Is represented (structured cot): {'steps': [{'step_number': 1, 'thought': 'Identify the main topic of the question and response.'}, {'step_number': 2, 'thought': 'Determine if the opinion is directly related to the main topic.'}, {'step_number': 3, 'thought': 'Evaluate if the opinion is supported by the information in the response.'}], 'relevance_score': 0}\n",
      "\n",
      "Entailment result: [(0, 186), (421, 593)]\n",
      "\n",
      "Entailment Reasoning: {'steps': [{'step_number': 1, 'thought': 'The opinion states that the question of extending human life by 50 years is not easy to answer. I need to find a similar sentiment in the Rich Text.'}, {'step_number': 2, 'thought': 'The Rich Text begins with \"This question is highly subjective and there are various ethical, social, and scientific considerations to take into account when discussing the idea of extending human life by 50 years.\" This matches the opinion\\'s sentiment that the question is not easy to answer.'}, {'step_number': 3, 'thought': 'The opinion mentions that the quality of life if extended is a concern, particularly if it involves illness and disease. I need to find a similar concern in the Rich Text.'}, {'step_number': 4, 'thought': 'The Rich Text does not explicitly mention quality of life, illness, or disease, so there is no direct match for this part of the opinion.'}, {'step_number': 5, 'thought': 'The opinion states that extending life for everyone would have massive social consequences requiring analysis. I need to find a similar concern in the Rich Text.'}, {'step_number': 6, 'thought': 'The Rich Text mentions \"concerns may be raised about overpopulation, resource depletion, inequality in access to life-extending technologies, and the potential negative impacts on the environment.\" This matches the opinion\\'s concern about massive social consequences.'}], 'final_answer': {'matches': ['This question is highly subjective and there are various ethical, social, and scientific considerations to take into account when discussing the idea of extending human life by 50 years.', 'concerns may be raised about overpopulation, resource depletion, inequality in access to life-extending technologies, and the potential negative impacts on the environment.']}}\n",
      "\n",
      "META ANALYSIS:\n",
      "\n",
      "Simple prompt == Structured cot: False\n",
      "\n",
      "Entailment length > 1 AND simple prompt == structured cot: False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Question: {example_question['question.text']}\n",
    "      \n",
    "Response: {response}\n",
    "\n",
    "From model: {example['model']}\n",
    "\n",
    "Opinion: {opinion}\n",
    "\n",
    "ANALYSIS:\n",
    "\n",
    "Is represented (simple prompt): {example['is_represented_simple_prompt']}\n",
    "\n",
    "Is represented (structured cot): {example['is_represented_structured_cot']}\n",
    "\n",
    "Entailment result: {example['entailment_matches']}\n",
    "\n",
    "Entailment Reasoning: {example['entailment_result']}\n",
    "\n",
    "META ANALYSIS:\n",
    "\n",
    "Simple prompt == Structured cot: {example['meta_analysis.simpleXstructured'] == 1}\n",
    "\n",
    "Entailment length > 1 AND simple prompt == structured cot: {example['meta_analysis.entailmentXstructured']}\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
